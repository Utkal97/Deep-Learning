{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61754cf5",
   "metadata": {
    "id": "61754cf5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fce1a529c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5RckaQtOPzcH",
   "metadata": {
    "id": "5RckaQtOPzcH",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cosineFunction(x):\n",
    "  return (np.cos(5 * np.pi * x))\n",
    "\n",
    "x = np.linspace(-1, 1, 300).reshape(-1, 1)\n",
    "y = cosineFunction(x)\n",
    "func1_x, func1_y = torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "plt.plot(func1_x, func1_y)\n",
    "plt.title(r'${\\cos(5 \\pi x)}$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c60c80",
   "metadata": {
    "id": "16c60c80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer_1 = nn.Linear(1, 5)\n",
    "        self.layer_2 = nn.Linear(5, 15)\n",
    "        self.layer_3 = nn.Linear(15, 15)\n",
    "        self.layer_4 = nn.Linear(15, 15)\n",
    "        self.layer_5 = nn.Linear(15, 15)\n",
    "        self.layer_6 = nn.Linear(15, 15)\n",
    "        self.layer_7 = nn.Linear(15, 15)\n",
    "        self.layer_8 = nn.Linear(15, 5)\n",
    "        self.layer_9 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, inputData):\n",
    "        layerOut1 = nn.functional.relu(self.layer_1(inputData))\n",
    "        layerOut2 = nn.functional.relu(self.layer_2(layerOut1))\n",
    "        layerOut3 = nn.functional.relu(self.layer_3(layerOut2))\n",
    "        layerOut4 = nn.functional.relu(self.layer_4(layerOut3))\n",
    "        layerOut5 = nn.functional.relu(self.layer_5(layerOut4))\n",
    "        layerOut6 = nn.functional.relu(self.layer_6(layerOut5))\n",
    "        layerOut7 = nn.functional.relu(self.layer_7(layerOut6))\n",
    "        layerOut8 = nn.functional.relu(self.layer_8(layerOut7))\n",
    "        layerOut9 = self.layer_9(layerOut8)\n",
    "        return layerOut9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fb27b97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fb27b97",
    "outputId": "64e1753e-c5a2-4d0e-c2a3-9217f208b9a0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params -  1386\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total params - ', total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c86ec1f4",
   "metadata": {
    "id": "c86ec1f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, inputData, groundTruth):\n",
    "    losses = []\n",
    "    epoch = 0\n",
    "    totalEpochs = 20000\n",
    "    acceptableLoss = 0.0001\n",
    "    acceptableGradNorm = 0.001\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "    isConverged = epoch >= totalEpochs\n",
    "\n",
    "    print(f'Training for {totalEpochs} epochs until Loss goes below {acceptableGradNorm}')\n",
    "\n",
    "    while not isConverged:\n",
    "        model.train()\n",
    "        predictedOutput = model(inputData)  \n",
    "        loss = nn.MSELoss()(predictedOutput, groundTruth)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step() \n",
    "        currentLoss = loss.detach().numpy()\n",
    "        losses.append(currentLoss)\n",
    "\n",
    "        gradSum = 0.0\n",
    "        for p in model.parameters():\n",
    "            grad =  (p.grad.cpu().data.numpy()**2).sum() if (p.grad is not None) else 0.0\n",
    "            gradSum += grad\n",
    "        gradNorm = gradSum ** 0.5\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'epoch: {epoch} currentLoss: {currentLoss} gradientNorm: {gradNorm}')\n",
    "\n",
    "        epoch += 1\n",
    "        isConverged = epoch >= totalEpochs or gradNorm < acceptableGradNorm\n",
    "\n",
    "    minimal_ratio, loss_val = calculate_hessian(model, inputData, groundTruth)\n",
    "    print(f\"epoch: {epoch} Minimal Ratio: {minimal_ratio}  Loss: {loss_val}\\n\")\n",
    "    return minimal_ratio, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "D_zoVRpMSCCW",
   "metadata": {
    "id": "D_zoVRpMSCCW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_hessian(model, x, y):\n",
    "    criterion = nn.MSELoss()\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    predictions = model(x)\n",
    "    loss = criterion(predictions, y)\n",
    "\n",
    "    Jacob = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    Jacob_flat = torch.cat([e.flatten() for e in Jacob])\n",
    "\n",
    "    H = torch.zeros((total_params, total_params))\n",
    "\n",
    "    for i in range(total_params):\n",
    "        result = torch.autograd.grad(Jacob_flat[i], model.parameters(), retain_graph=True)\n",
    "        H[i] = torch.cat([r.flatten() for r in result])\n",
    "\n",
    "    H_eigenvalues = torch.linalg.eig(H).eigenvalues.real\n",
    "    positive_count = (H_eigenvalues > 0).sum().item()\n",
    "    minimal_ratio = positive_count / total_params\n",
    "\n",
    "    return minimal_ratio, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a385dad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a385dad",
    "outputId": "3ac72d5e-1121-4cf1-a87c-07339945f4f8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model  0\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.47795867919921875 gradientNorm: 0.8608432700845996\n",
      "epoch: 1000 currentLoss: 0.018095476552844048 gradientNorm: 0.07189023128696759\n",
      "epoch: 2000 currentLoss: 0.00034788966877385974 gradientNorm: 0.006115735514192596\n",
      "epoch: 2951 Minimal Ratio: 0.25757575757575757  Loss: 3.583202123991214e-05\n",
      "\n",
      "Training model  1\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4282243847846985 gradientNorm: 0.6761848884897569\n",
      "epoch: 1000 currentLoss: 0.03236721456050873 gradientNorm: 0.20851475409815107\n",
      "epoch: 1877 Minimal Ratio: 0.2784992784992785  Loss: 9.17208944883896e-06\n",
      "\n",
      "Training model  2\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3592028319835663 gradientNorm: 0.3792414633529094\n",
      "epoch: 1000 currentLoss: 0.14070381224155426 gradientNorm: 0.4215136750342594\n",
      "epoch: 2000 currentLoss: 0.09265583753585815 gradientNorm: 0.33670060833703164\n",
      "epoch: 3000 currentLoss: 0.06542601436376572 gradientNorm: 0.26677177638210975\n",
      "epoch: 4000 currentLoss: 0.045806027948856354 gradientNorm: 0.2103814831622924\n",
      "epoch: 5000 currentLoss: 0.03130383789539337 gradientNorm: 0.16353167225097445\n",
      "epoch: 6000 currentLoss: 0.020701685920357704 gradientNorm: 0.12448931404923272\n",
      "epoch: 7000 currentLoss: 0.013124516233801842 gradientNorm: 0.09168000539675891\n",
      "epoch: 8000 currentLoss: 0.007903184741735458 gradientNorm: 0.06529504034403229\n",
      "epoch: 9000 currentLoss: 0.004479048307985067 gradientNorm: 0.044651235759463025\n",
      "epoch: 10000 currentLoss: 0.0023619725834578276 gradientNorm: 0.029099746344761954\n",
      "epoch: 11000 currentLoss: 0.0011452543549239635 gradientNorm: 0.018030256102491007\n",
      "epoch: 12000 currentLoss: 0.0005046140868216753 gradientNorm: 0.010788994977903216\n",
      "epoch: 13000 currentLoss: 0.00020007604325655848 gradientNorm: 0.005532696737533105\n",
      "epoch: 14000 currentLoss: 7.110342994565144e-05 gradientNorm: 0.0027673392476635916\n",
      "epoch: 15000 currentLoss: 2.2839580196887255e-05 gradientNorm: 0.0020001746741145324\n",
      "epoch: 15301 Minimal Ratio: 0.23737373737373738  Loss: 1.5855515812290832e-05\n",
      "\n",
      "Training model  3\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.44485482573509216 gradientNorm: 0.6706025155822778\n",
      "epoch: 1000 currentLoss: 0.3875533938407898 gradientNorm: 0.45602828609005536\n",
      "epoch: 2000 currentLoss: 0.35783591866493225 gradientNorm: 0.2984823832289438\n",
      "epoch: 3000 currentLoss: 0.34276527166366577 gradientNorm: 0.1697322401144085\n",
      "epoch: 4000 currentLoss: 0.3369773328304291 gradientNorm: 0.0752160706292482\n",
      "epoch: 5000 currentLoss: 0.33567488193511963 gradientNorm: 0.02115285028655113\n",
      "epoch: 6000 currentLoss: 0.3355647325515747 gradientNorm: 0.0026381020841154154\n",
      "epoch: 6336 Minimal Ratio: 0.0007215007215007215  Loss: 0.33556321263313293\n",
      "\n",
      "Training model  4\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4775245189666748 gradientNorm: 0.9668377846718257\n",
      "epoch: 1000 currentLoss: 0.0441969633102417 gradientNorm: 0.29685174891552063\n",
      "epoch: 2000 currentLoss: 3.988230673712678e-05 gradientNorm: 0.001816000934841687\n",
      "epoch: 2148 Minimal Ratio: 0.22582972582972582  Loss: 2.806299744406715e-05\n",
      "\n",
      "Training model  5\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.40971100330352783 gradientNorm: 0.5744335670575806\n",
      "epoch: 1000 currentLoss: 0.06977248936891556 gradientNorm: 0.2875552104693478\n",
      "epoch: 2000 currentLoss: 0.02387065812945366 gradientNorm: 0.15902815456129882\n",
      "epoch: 3000 currentLoss: 0.0010360591113567352 gradientNorm: 0.011695295204981488\n",
      "epoch: 4000 currentLoss: 2.3601505745318718e-05 gradientNorm: 0.0010188652931868508\n",
      "epoch: 4009 Minimal Ratio: 0.2453102453102453  Loss: 2.2964304662309587e-05\n",
      "\n",
      "Training model  6\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.33568382263183594 gradientNorm: 0.011505813725648335\n",
      "epoch: 1000 currentLoss: 0.017345357686281204 gradientNorm: 0.16233657416551694\n",
      "epoch: 2000 currentLoss: 0.00019731326028704643 gradientNorm: 0.0030260882765753053\n",
      "epoch: 3000 currentLoss: 4.1541548853274435e-05 gradientNorm: 0.0012006377089222985\n",
      "epoch: 3136 Minimal Ratio: 0.2611832611832612  Loss: 3.3151569368783385e-05\n",
      "\n",
      "Training model  7\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3375622630119324 gradientNorm: 0.10688153303652631\n",
      "epoch: 1000 currentLoss: 0.0825342908501625 gradientNorm: 0.3102039177020483\n",
      "epoch: 2000 currentLoss: 0.05235832929611206 gradientNorm: 0.23051467812087956\n",
      "epoch: 3000 currentLoss: 0.03479583188891411 gradientNorm: 0.17566680211788369\n",
      "epoch: 4000 currentLoss: 0.022777117788791656 gradientNorm: 0.13248873760504842\n",
      "epoch: 5000 currentLoss: 0.014426044188439846 gradientNorm: 0.09761390701273562\n",
      "epoch: 6000 currentLoss: 0.008730516768991947 gradientNorm: 0.07052365085245316\n",
      "epoch: 7000 currentLoss: 0.004989760462194681 gradientNorm: 0.04825407947078881\n",
      "epoch: 8000 currentLoss: 0.002661497099325061 gradientNorm: 0.03205287021673773\n",
      "epoch: 9000 currentLoss: 0.0013081550132483244 gradientNorm: 0.020635766336395352\n",
      "epoch: 10000 currentLoss: 0.0005851030582562089 gradientNorm: 0.011434176513986836\n",
      "epoch: 11000 currentLoss: 0.00023557119129691273 gradientNorm: 0.006216737664872466\n",
      "epoch: 12000 currentLoss: 8.499290561303496e-05 gradientNorm: 0.004190068785301306\n",
      "epoch: 13000 currentLoss: 2.755630157480482e-05 gradientNorm: 0.0019975496506294205\n",
      "epoch: 13578 Minimal Ratio: 0.26262626262626265  Loss: 1.3743471754423808e-05\n",
      "\n",
      "Training model  8\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.38263800740242004 gradientNorm: 0.48379520478060106\n",
      "epoch: 1000 currentLoss: 0.0676044374704361 gradientNorm: 0.28954670708595004\n",
      "epoch: 2000 currentLoss: 0.039667315781116486 gradientNorm: 0.1902808564768697\n",
      "epoch: 3000 currentLoss: 0.0260671004652977 gradientNorm: 0.14441437943445526\n",
      "epoch: 4000 currentLoss: 0.016634603962302208 gradientNorm: 0.10718362358952778\n",
      "epoch: 5000 currentLoss: 0.01018119789659977 gradientNorm: 0.07731736839790736\n",
      "epoch: 6000 currentLoss: 0.005910094361752272 gradientNorm: 0.05383980516776225\n",
      "epoch: 7000 currentLoss: 0.0032142396084964275 gradientNorm: 0.03584916381825822\n",
      "epoch: 8000 currentLoss: 0.0016168291913345456 gradientNorm: 0.022833333840910432\n",
      "epoch: 9000 currentLoss: 0.0007431363337673247 gradientNorm: 0.013329396923056439\n",
      "epoch: 10000 currentLoss: 0.00030858878744766116 gradientNorm: 0.007531858635706477\n",
      "epoch: 11000 currentLoss: 0.00011547416215762496 gradientNorm: 0.008912162461685377\n",
      "epoch: 12000 currentLoss: 3.833868322544731e-05 gradientNorm: 0.001894054245881468\n",
      "epoch: 12787 Minimal Ratio: 0.2582972582972583  Loss: 1.5033395357022528e-05\n",
      "\n",
      "Training model  9\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5141458511352539 gradientNorm: 0.9589858015644143\n",
      "epoch: 1000 currentLoss: 0.044325947761535645 gradientNorm: 0.21419186391263068\n",
      "epoch: 2000 currentLoss: 0.00013523994130082428 gradientNorm: 0.004406007691933765\n",
      "epoch: 2698 Minimal Ratio: 0.30808080808080807  Loss: 2.848502845154144e-05\n",
      "\n",
      "Training model  10\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.40277817845344543 gradientNorm: 0.5326881760491787\n",
      "epoch: 1000 currentLoss: 0.3556862771511078 gradientNorm: 0.31620432834087153\n",
      "epoch: 2000 currentLoss: 0.0004810488608200103 gradientNorm: 0.006790525767726038\n",
      "epoch: 3000 currentLoss: 7.55317450966686e-05 gradientNorm: 0.0030825556609800353\n",
      "epoch: 3547 Minimal Ratio: 0.23665223665223664  Loss: 2.6486417482374236e-05\n",
      "\n",
      "Training model  11\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.49362432956695557 gradientNorm: 0.7951385427712062\n",
      "epoch: 1000 currentLoss: 0.42752954363822937 gradientNorm: 0.6065198081635109\n",
      "epoch: 2000 currentLoss: 0.38384759426116943 gradientNorm: 0.439475181873765\n",
      "epoch: 3000 currentLoss: 0.3570586144924164 gradientNorm: 0.29322755190922467\n",
      "epoch: 4000 currentLoss: 0.34283873438835144 gradientNorm: 0.17059657003234963\n",
      "epoch: 5000 currentLoss: 0.33709070086479187 gradientNorm: 0.07817259471237181\n",
      "epoch: 6000 currentLoss: 0.3356984853744507 gradientNorm: 0.02328386216902495\n",
      "epoch: 7000 currentLoss: 0.33556559681892395 gradientNorm: 0.003214485440157402\n",
      "epoch: 7416 Minimal Ratio: 0.0007215007215007215  Loss: 0.33556321263313293\n",
      "\n",
      "Training model  12\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.34641245007514954 gradientNorm: 0.2386150228043684\n",
      "epoch: 1000 currentLoss: 0.0417400486767292 gradientNorm: 0.28192573117543895\n",
      "epoch: 2000 currentLoss: 0.000116059607535135 gradientNorm: 0.0037918637148563095\n",
      "epoch: 2574 Minimal Ratio: 0.2619047619047619  Loss: 8.20232799014775e-06\n",
      "\n",
      "Training model  13\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.34991732239723206 gradientNorm: 0.2552456307343356\n",
      "epoch: 1000 currentLoss: 0.009637213312089443 gradientNorm: 0.11213326829669028\n",
      "epoch: 2000 currentLoss: 0.0001196649027406238 gradientNorm: 0.004039381142919532\n",
      "epoch: 2842 Minimal Ratio: 0.2554112554112554  Loss: 1.758903636073228e-05\n",
      "\n",
      "Training model  14\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.35226917266845703 gradientNorm: 0.30471957616747625\n",
      "epoch: 1000 currentLoss: 0.010476083494722843 gradientNorm: 0.12657053282569422\n",
      "epoch: 2000 currentLoss: 1.751182571751997e-05 gradientNorm: 0.002857193160353184\n",
      "epoch: 2256 Minimal Ratio: 0.2178932178932179  Loss: 4.273833383194869e-06\n",
      "\n",
      "Training model  15\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3469560146331787 gradientNorm: 0.22030996848316958\n",
      "epoch: 1000 currentLoss: 0.10329942405223846 gradientNorm: 0.35616511803995893\n",
      "epoch: 2000 currentLoss: 0.06699264794588089 gradientNorm: 0.27152008020899265\n",
      "epoch: 3000 currentLoss: 0.04573056101799011 gradientNorm: 0.21071879326634174\n",
      "epoch: 4000 currentLoss: 0.03084418550133705 gradientNorm: 0.16212252036261693\n",
      "epoch: 5000 currentLoss: 0.020210500806570053 gradientNorm: 0.12233378245091976\n",
      "epoch: 6000 currentLoss: 0.012721681967377663 gradientNorm: 0.0898459229457917\n",
      "epoch: 7000 currentLoss: 0.007614995818585157 gradientNorm: 0.06379685070627412\n",
      "epoch: 8000 currentLoss: 0.004287737421691418 gradientNorm: 0.04350914447951927\n",
      "epoch: 9000 currentLoss: 0.0022447751834988594 gradientNorm: 0.02835768684407308\n",
      "epoch: 10000 currentLoss: 0.0010794574627652764 gradientNorm: 0.01843126260559641\n",
      "epoch: 11000 currentLoss: 0.0004709489003289491 gradientNorm: 0.009969541550568142\n",
      "epoch: 12000 currentLoss: 0.00018472618830855936 gradientNorm: 0.005782750734516887\n",
      "epoch: 13000 currentLoss: 6.484785990323871e-05 gradientNorm: 0.0026859584679966076\n",
      "epoch: 14000 currentLoss: 2.0464460249058902e-05 gradientNorm: 0.00135282414098749\n",
      "epoch: 14256 Minimal Ratio: 0.1976911976911977  Loss: 1.4986459063948132e-05\n",
      "\n",
      "Training model  16\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3778749704360962 gradientNorm: 0.5083390743843685\n",
      "epoch: 1000 currentLoss: 0.14669594168663025 gradientNorm: 0.4531474427046058\n",
      "epoch: 2000 currentLoss: 0.09997954964637756 gradientNorm: 0.3543317272630638\n",
      "epoch: 3000 currentLoss: 0.07135580480098724 gradientNorm: 0.28284894962574175\n",
      "epoch: 4000 currentLoss: 0.0504252053797245 gradientNorm: 0.22459346504949745\n",
      "epoch: 5000 currentLoss: 0.034807413816452026 gradientNorm: 0.17553280690617504\n",
      "epoch: 6000 currentLoss: 0.02326093800365925 gradientNorm: 0.13428196289325356\n",
      "epoch: 7000 currentLoss: 0.014927460812032223 gradientNorm: 0.0999705212262132\n",
      "epoch: 8000 currentLoss: 0.00912418495863676 gradientNorm: 0.07205520726269699\n",
      "epoch: 9000 currentLoss: 0.0052614640444517136 gradientNorm: 0.049858929473575404\n",
      "epoch: 10000 currentLoss: 0.0028313701041042805 gradientNorm: 0.032971691581962365\n",
      "epoch: 11000 currentLoss: 0.0014050164027139544 gradientNorm: 0.020649454777844193\n",
      "epoch: 12000 currentLoss: 0.0006351727060973644 gradientNorm: 0.012179258050989596\n",
      "epoch: 13000 currentLoss: 0.00025885546347126365 gradientNorm: 0.006646721983850742\n",
      "epoch: 14000 currentLoss: 9.450793731957674e-05 gradientNorm: 0.00341489877265812\n",
      "epoch: 15000 currentLoss: 3.0939350835978985e-05 gradientNorm: 0.0016530495335858586\n",
      "epoch: 15578 Minimal Ratio: 0.22438672438672438  Loss: 1.5488845747313462e-05\n",
      "\n",
      "Training model  17\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4965663552284241 gradientNorm: 0.8606467270364385\n",
      "epoch: 1000 currentLoss: 0.0472661629319191 gradientNorm: 0.19848834635147197\n",
      "epoch: 2000 currentLoss: 0.030331039801239967 gradientNorm: 0.15418169329902212\n",
      "epoch: 3000 currentLoss: 0.020474698394536972 gradientNorm: 0.12168980318487821\n",
      "epoch: 4000 currentLoss: 0.013197271153330803 gradientNorm: 0.09129077557257445\n",
      "epoch: 5000 currentLoss: 0.008026336319744587 gradientNorm: 0.06576459273440584\n",
      "epoch: 6000 currentLoss: 0.004576078150421381 gradientNorm: 0.04538624772975514\n",
      "epoch: 7000 currentLoss: 0.002424095058813691 gradientNorm: 0.02965349751497746\n",
      "epoch: 8000 currentLoss: 0.0011796632315963507 gradientNorm: 0.01831565116598344\n",
      "epoch: 9000 currentLoss: 0.0005214670090936124 gradientNorm: 0.010589725090289326\n",
      "epoch: 10000 currentLoss: 0.0002074158110190183 gradientNorm: 0.005950823651637369\n",
      "epoch: 11000 currentLoss: 7.399632886517793e-05 gradientNorm: 0.0038994392668781166\n",
      "epoch: 12000 currentLoss: 2.3707867512712255e-05 gradientNorm: 0.0013635222249748229\n",
      "epoch: 12380 Minimal Ratio: 0.227994227994228  Loss: 1.5004285160102881e-05\n",
      "\n",
      "Training model  18\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.38377371430397034 gradientNorm: 0.4528185360153331\n",
      "epoch: 1000 currentLoss: 0.07393253594636917 gradientNorm: 0.2879249728995058\n",
      "epoch: 2000 currentLoss: 0.0489511638879776 gradientNorm: 0.22017035314989933\n",
      "epoch: 3000 currentLoss: 0.03272715210914612 gradientNorm: 0.1684936658278996\n",
      "epoch: 4000 currentLoss: 0.021390531212091446 gradientNorm: 0.12693524022049896\n",
      "epoch: 5000 currentLoss: 0.01347997784614563 gradientNorm: 0.09333555130220852\n",
      "epoch: 6000 currentLoss: 0.008097351528704166 gradientNorm: 0.06646389961392855\n",
      "epoch: 7000 currentLoss: 0.004585261456668377 gradientNorm: 0.045493752803714434\n",
      "epoch: 8000 currentLoss: 0.0024193867575377226 gradientNorm: 0.029702648600084973\n",
      "epoch: 9000 currentLoss: 0.001174922101199627 gradientNorm: 0.018293790193276642\n",
      "epoch: 10000 currentLoss: 0.0005187482456676662 gradientNorm: 0.011213280223996156\n",
      "epoch: 11000 currentLoss: 0.00020604796009138227 gradientNorm: 0.005687240434522503\n",
      "epoch: 12000 currentLoss: 7.38773524062708e-05 gradientNorm: 0.008103921712113401\n",
      "epoch: 13000 currentLoss: 2.3420609068125486e-05 gradientNorm: 0.002330349677049119\n",
      "epoch: 13372 Minimal Ratio: 0.2113997113997114  Loss: 1.4915212886990048e-05\n",
      "\n",
      "Training model  19\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5225604176521301 gradientNorm: 1.0763609986060143\n",
      "epoch: 1000 currentLoss: 0.001603630487807095 gradientNorm: 0.06725926610953455\n",
      "epoch: 1857 Minimal Ratio: 0.30735930735930733  Loss: 3.9834307244746014e-05\n",
      "\n",
      "Training model  20\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.33882227540016174 gradientNorm: 0.11542160205978329\n",
      "epoch: 1000 currentLoss: 0.005403235554695129 gradientNorm: 0.07801629086215461\n",
      "epoch: 2000 currentLoss: 0.00011881613318109885 gradientNorm: 0.0019235929316802978\n",
      "epoch: 2472 Minimal Ratio: 0.3116883116883117  Loss: 4.85811397084035e-05\n",
      "\n",
      "Training model  21\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.36580297350883484 gradientNorm: 0.34779298429413585\n",
      "epoch: 1000 currentLoss: 0.3432731032371521 gradientNorm: 0.17561474437567837\n",
      "epoch: 2000 currentLoss: 0.33656880259513855 gradientNorm: 0.06342971980722156\n",
      "epoch: 3000 currentLoss: 0.33559924364089966 gradientNorm: 0.012038726459485134\n",
      "epoch: 3910 Minimal Ratio: 0.0007215007215007215  Loss: 0.33556321263313293\n",
      "\n",
      "Training model  22\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.34330514073371887 gradientNorm: 0.22764295767010959\n",
      "epoch: 1000 currentLoss: 0.0013676445232704282 gradientNorm: 0.03461645196518717\n",
      "epoch: 2000 currentLoss: 5.2461538871284574e-05 gradientNorm: 0.0011556225248104365\n",
      "epoch: 2110 Minimal Ratio: 0.3008658008658009  Loss: 4.395467840367928e-05\n",
      "\n",
      "Training model  23\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4493643641471863 gradientNorm: 0.8399049938991294\n",
      "epoch: 1000 currentLoss: 0.06907430291175842 gradientNorm: 0.3471767154852086\n",
      "epoch: 2000 currentLoss: 0.013095238246023655 gradientNorm: 0.12969331924752547\n",
      "epoch: 3000 Minimal Ratio: 0.27056277056277056  Loss: 1.4469242159975693e-05\n",
      "\n",
      "Training model  24\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.33596745133399963 gradientNorm: 0.05500502844205406\n",
      "epoch: 1000 currentLoss: 0.023541338741779327 gradientNorm: 0.19112015339655367\n",
      "epoch: 2000 currentLoss: 0.0004724446916952729 gradientNorm: 0.006130276922166529\n",
      "epoch: 3000 currentLoss: 8.078222890617326e-05 gradientNorm: 0.0014402952903343872\n",
      "epoch: 3217 Minimal Ratio: 0.29797979797979796  Loss: 5.3070976719027385e-05\n",
      "\n",
      "Training model  25\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.42411479353904724 gradientNorm: 0.6796900957469901\n",
      "epoch: 1000 currentLoss: 0.06876406073570251 gradientNorm: 0.3294789699985417\n",
      "epoch: 2000 currentLoss: 7.863026257837191e-05 gradientNorm: 0.00330419588357617\n",
      "epoch: 2392 Minimal Ratio: 0.2676767676767677  Loss: 1.7887994545162655e-05\n",
      "\n",
      "Training model  26\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.34070461988449097 gradientNorm: 0.19295666300728406\n",
      "epoch: 1000 currentLoss: 0.002667980268597603 gradientNorm: 0.033794709763856096\n",
      "epoch: 2000 currentLoss: 1.6214547940762714e-05 gradientNorm: 0.0011662409461570585\n",
      "epoch: 2046 Minimal Ratio: 0.27200577200577203  Loss: 1.4273320630309172e-05\n",
      "\n",
      "Training model  27\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3475722670555115 gradientNorm: 0.23120241300415934\n",
      "epoch: 1000 currentLoss: 0.14556749165058136 gradientNorm: 0.4545614169937183\n",
      "epoch: 2000 currentLoss: 0.09864698350429535 gradientNorm: 0.3493618928678195\n",
      "epoch: 3000 currentLoss: 0.07010868936777115 gradientNorm: 0.2792515590669972\n",
      "epoch: 4000 currentLoss: 0.04938323050737381 gradientNorm: 0.2214568973911243\n",
      "epoch: 5000 currentLoss: 0.03400099650025368 gradientNorm: 0.17286132728612344\n",
      "epoch: 6000 currentLoss: 0.022662006318569183 gradientNorm: 0.13190825813052282\n",
      "epoch: 7000 currentLoss: 0.014503682032227516 gradientNorm: 0.09800022178615239\n",
      "epoch: 8000 currentLoss: 0.00883565191179514 gradientNorm: 0.07043273211309276\n",
      "epoch: 9000 currentLoss: 0.005074326880276203 gradientNorm: 0.048673417297398894\n",
      "epoch: 10000 currentLoss: 0.0027173797134310007 gradientNorm: 0.03193558142378614\n",
      "epoch: 11000 currentLoss: 0.001340817310847342 gradientNorm: 0.019934315332388194\n",
      "epoch: 12000 currentLoss: 0.0006022484740242362 gradientNorm: 0.01166015441287649\n",
      "epoch: 13000 currentLoss: 0.00024368322920054197 gradientNorm: 0.006386784849219798\n",
      "epoch: 14000 currentLoss: 8.829798025544733e-05 gradientNorm: 0.003208199704280265\n",
      "epoch: 15000 currentLoss: 2.8688205929938704e-05 gradientNorm: 0.001751723574381345\n",
      "epoch: 15554 Minimal Ratio: 0.22366522366522368  Loss: 1.4716956684424076e-05\n",
      "\n",
      "Training model  28\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.567344069480896 gradientNorm: 1.248606696785099\n",
      "epoch: 1000 currentLoss: 0.019008507952094078 gradientNorm: 0.24356795431165676\n",
      "epoch: 2000 currentLoss: 5.538627374335192e-05 gradientNorm: 0.0021524529748225387\n",
      "epoch: 2404 Minimal Ratio: 0.31457431457431456  Loss: 3.004558129759971e-05\n",
      "\n",
      "Training model  29\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4036540687084198 gradientNorm: 0.5923835939851089\n",
      "epoch: 1000 currentLoss: 0.010203782469034195 gradientNorm: 0.05765221717667338\n",
      "epoch: 2000 currentLoss: 5.519744081539102e-05 gradientNorm: 0.0025299409075869124\n",
      "epoch: 2310 Minimal Ratio: 0.24603174603174602  Loss: 1.1814421668532304e-05\n",
      "\n",
      "Training model  30\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.33987560868263245 gradientNorm: 0.1501705319453078\n",
      "epoch: 1000 currentLoss: 0.041868217289447784 gradientNorm: 0.29334450064467715\n",
      "epoch: 2000 currentLoss: 6.275642954278737e-05 gradientNorm: 0.0034271055266412134\n",
      "epoch: 2453 Minimal Ratio: 0.19913419913419914  Loss: 1.1515301594045013e-05\n",
      "\n",
      "Training model  31\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.33831873536109924 gradientNorm: 0.12226768352648462\n",
      "epoch: 1000 currentLoss: 0.006621573586016893 gradientNorm: 0.10731648951859574\n",
      "epoch: 2000 currentLoss: 5.002578109269962e-05 gradientNorm: 0.0017877051963871805\n",
      "epoch: 2441 Minimal Ratio: 0.2647907647907648  Loss: 2.546406540204771e-05\n",
      "\n",
      "Training model  32\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4783553183078766 gradientNorm: 0.7762144927542542\n",
      "epoch: 1000 currentLoss: 0.026990363374352455 gradientNorm: 0.22237506316430145\n",
      "epoch: 2000 currentLoss: 0.00014841710799373686 gradientNorm: 0.002773072851891726\n",
      "epoch: 2554 Minimal Ratio: 0.2857142857142857  Loss: 3.538611781550571e-05\n",
      "\n",
      "Training model  33\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3773253858089447 gradientNorm: 0.5038136521380879\n",
      "epoch: 1000 currentLoss: 0.012592313811182976 gradientNorm: 0.12365468321850842\n",
      "epoch: 2000 currentLoss: 8.675231219967827e-05 gradientNorm: 0.001758688629887426\n",
      "epoch: 2440 Minimal Ratio: 0.2972582972582973  Loss: 5.0459057092666626e-05\n",
      "\n",
      "Training model  34\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5110383033752441 gradientNorm: 0.8894516169228731\n",
      "epoch: 1000 currentLoss: 0.40425705909729004 gradientNorm: 0.5710031026321809\n",
      "epoch: 2000 currentLoss: 0.27190738916397095 gradientNorm: 0.6915632981534395\n",
      "epoch: 3000 currentLoss: 0.20908556878566742 gradientNorm: 0.5808355792913255\n",
      "epoch: 4000 currentLoss: 0.16146785020828247 gradientNorm: 0.4889642229818371\n",
      "epoch: 5000 currentLoss: 0.12340408563613892 gradientNorm: 0.40874109167012357\n",
      "epoch: 6000 currentLoss: 0.09272371232509613 gradientNorm: 0.3378114174900048\n",
      "epoch: 7000 currentLoss: 0.06817688792943954 gradientNorm: 0.2753170542290825\n",
      "epoch: 8000 currentLoss: 0.048840343952178955 gradientNorm: 0.22060357012696344\n",
      "epoch: 9000 currentLoss: 0.03393029421567917 gradientNorm: 0.17291773617699366\n",
      "epoch: 10000 currentLoss: 0.022736547514796257 gradientNorm: 0.1323308396130111\n",
      "epoch: 11000 currentLoss: 0.014601907692849636 gradientNorm: 0.09854535250036342\n",
      "epoch: 12000 currentLoss: 0.00891834031790495 gradientNorm: 0.07091608336977842\n",
      "epoch: 13000 currentLoss: 0.005133866798132658 gradientNorm: 0.05086319749454951\n",
      "epoch: 14000 currentLoss: 0.002755633322522044 gradientNorm: 0.03174337036726039\n",
      "epoch: 15000 currentLoss: 0.0013625380815938115 gradientNorm: 0.020267153931597677\n",
      "epoch: 16000 currentLoss: 0.000613572949077934 gradientNorm: 0.012373837154957826\n",
      "epoch: 17000 currentLoss: 0.0002491694176569581 gradientNorm: 0.006738843730990849\n",
      "epoch: 18000 currentLoss: 9.055251575773582e-05 gradientNorm: 0.003287650331008706\n",
      "epoch: 19000 currentLoss: 2.9540924515458755e-05 gradientNorm: 0.0016845953940020303\n",
      "epoch: 19553 Minimal Ratio: 0.2619047619047619  Loss: 1.5213448932627216e-05\n",
      "\n",
      "Training model  35\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.44787272810935974 gradientNorm: 0.7984947337637723\n",
      "epoch: 1000 currentLoss: 0.22343197464942932 gradientNorm: 0.4378525762998134\n",
      "epoch: 2000 currentLoss: 0.00044967979192733765 gradientNorm: 0.008088824749133116\n",
      "epoch: 2940 Minimal Ratio: 0.2352092352092352  Loss: 2.2428352167480625e-05\n",
      "\n",
      "Training model  36\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5274195671081543 gradientNorm: 0.8760287844466337\n",
      "epoch: 1000 currentLoss: 0.4533165991306305 gradientNorm: 0.6863049177003337\n",
      "epoch: 2000 currentLoss: 0.40207231044769287 gradientNorm: 0.5157879658573536\n",
      "epoch: 3000 currentLoss: 0.3685435652732849 gradientNorm: 0.3632111635588355\n",
      "epoch: 4000 currentLoss: 0.34882652759552 gradientNorm: 0.23033488960598342\n",
      "epoch: 5000 currentLoss: 0.33931055665016174 gradientNorm: 0.12243439357802897\n",
      "epoch: 6000 currentLoss: 0.3361298441886902 gradientNorm: 0.04761671317961912\n",
      "epoch: 7000 currentLoss: 0.33559077978134155 gradientNorm: 0.010543837043543393\n",
      "epoch: 7957 Minimal Ratio: 0.0007215007215007215  Loss: 0.33556321263313293\n",
      "\n",
      "Training model  37\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3875732421875 gradientNorm: 0.5126027162011159\n",
      "epoch: 1000 currentLoss: 0.04962083324790001 gradientNorm: 0.27607935459530974\n",
      "epoch: 2000 currentLoss: 9.39760502660647e-05 gradientNorm: 0.0038207050155415396\n",
      "epoch: 2778 Minimal Ratio: 0.2597402597402597  Loss: 3.324215504107997e-05\n",
      "\n",
      "Training model  38\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3363719582557678 gradientNorm: 0.0772565678278421\n",
      "epoch: 1000 currentLoss: 0.004128545057028532 gradientNorm: 0.056261303149748074\n",
      "epoch: 2000 currentLoss: 0.00015719501243438572 gradientNorm: 0.0027190793851051558\n",
      "epoch: 2905 Minimal Ratio: 0.23376623376623376  Loss: 4.4206844904692844e-05\n",
      "\n",
      "Training model  39\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.36162373423576355 gradientNorm: 0.3294910108039897\n",
      "epoch: 1000 currentLoss: 0.07007700204849243 gradientNorm: 0.4106716832535915\n",
      "epoch: 2000 currentLoss: 1.7960175682674162e-05 gradientNorm: 0.001153399343750804\n",
      "epoch: 2040 Minimal Ratio: 0.2994227994227994  Loss: 1.6005238649086095e-05\n",
      "\n",
      "Training model  40\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5215316414833069 gradientNorm: 0.8988575660829009\n",
      "epoch: 1000 currentLoss: 0.12468606233596802 gradientNorm: 0.5279602800404644\n",
      "epoch: 2000 currentLoss: 7.3131764111167286e-06 gradientNorm: 0.0011233296227160482\n",
      "epoch: 2078 Minimal Ratio: 0.1406926406926407  Loss: 5.7331226344103925e-06\n",
      "\n",
      "Training model  41\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.36724674701690674 gradientNorm: 0.3559986608204138\n",
      "epoch: 1000 currentLoss: 0.3439455032348633 gradientNorm: 0.18311213163333204\n",
      "epoch: 2000 currentLoss: 0.33673831820487976 gradientNorm: 0.06856706608035519\n",
      "epoch: 3000 currentLoss: 0.3356117010116577 gradientNorm: 0.013961081147989203\n",
      "epoch: 3987 Minimal Ratio: 0.0007215007215007215  Loss: 0.33556321263313293\n",
      "\n",
      "Training model  42\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.36936670541763306 gradientNorm: 0.4082835491507265\n",
      "epoch: 1000 currentLoss: 0.18706688284873962 gradientNorm: 0.5329406933998477\n",
      "epoch: 2000 currentLoss: 0.13175073266029358 gradientNorm: 0.42657423743420025\n",
      "epoch: 3000 currentLoss: 0.09660263359546661 gradientNorm: 0.3470542751830423\n",
      "epoch: 4000 currentLoss: 0.07028030604124069 gradientNorm: 0.28078224438126054\n",
      "epoch: 5000 currentLoss: 0.050120994448661804 gradientNorm: 0.22414999264200483\n",
      "epoch: 6000 currentLoss: 0.034772735089063644 gradientNorm: 0.17565767813137692\n",
      "epoch: 7000 currentLoss: 0.023311050608754158 gradientNorm: 0.13457209439218892\n",
      "epoch: 8000 currentLoss: 0.014994346536695957 gradientNorm: 0.10018973489995639\n",
      "epoch: 8455 Minimal Ratio: 0.31601731601731603  Loss: 8.779258678259794e-06\n",
      "\n",
      "Training model  43\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4230440855026245 gradientNorm: 0.6049517574636297\n",
      "epoch: 1000 currentLoss: 0.24647684395313263 gradientNorm: 0.7196131895850505\n",
      "epoch: 2000 currentLoss: 0.06170764937996864 gradientNorm: 0.25520904680853945\n",
      "epoch: 3000 currentLoss: 0.04268508031964302 gradientNorm: 0.20110480625199129\n",
      "epoch: 4000 currentLoss: 0.02885640412569046 gradientNorm: 0.15503235850415553\n",
      "epoch: 5000 currentLoss: 0.01886070892214775 gradientNorm: 0.11681526502433506\n",
      "epoch: 6000 currentLoss: 0.011802857741713524 gradientNorm: 0.08543585179914416\n",
      "epoch: 7000 currentLoss: 0.007007040083408356 gradientNorm: 0.060361173650535255\n",
      "epoch: 8000 currentLoss: 0.0039061247371137142 gradientNorm: 0.0410372570876541\n",
      "epoch: 9000 currentLoss: 0.0020208011846989393 gradientNorm: 0.026396954707028835\n",
      "epoch: 10000 currentLoss: 0.0009582680650055408 gradientNorm: 0.016266473090622915\n",
      "epoch: 11000 currentLoss: 0.00041167964809574187 gradientNorm: 0.009914320329926523\n",
      "epoch: 12000 currentLoss: 0.00015883331070654094 gradientNorm: 0.006404917077932748\n",
      "epoch: 13000 currentLoss: 5.4827014537295327e-05 gradientNorm: 0.0032837654161376427\n",
      "epoch: 14000 currentLoss: 1.701250221231021e-05 gradientNorm: 0.0011065597516124322\n",
      "epoch: 14097 Minimal Ratio: 0.2041847041847042  Loss: 1.5104035810509231e-05\n",
      "\n",
      "Training model  44\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5235275030136108 gradientNorm: 0.9853327268260608\n",
      "epoch: 1000 currentLoss: 0.0761207863688469 gradientNorm: 0.9075275318808438\n",
      "epoch: 2000 currentLoss: 0.00021606033260468394 gradientNorm: 0.0031330926875884606\n",
      "epoch: 2725 Minimal Ratio: 0.27705627705627706  Loss: 0.0001108163851313293\n",
      "\n",
      "Training model  45\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4090298116207123 gradientNorm: 0.6198495659000189\n",
      "epoch: 1000 currentLoss: 0.04902375116944313 gradientNorm: 0.26734707352288595\n",
      "epoch: 2000 currentLoss: 0.00021252851001918316 gradientNorm: 0.0020953139271737945\n",
      "epoch: 2606 Minimal Ratio: 0.3246753246753247  Loss: 8.145954052451998e-05\n",
      "\n",
      "Training model  46\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3483080267906189 gradientNorm: 0.2428925163319257\n",
      "epoch: 1000 currentLoss: 0.03723173588514328 gradientNorm: 0.35154476681910624\n",
      "epoch: 1971 Minimal Ratio: 0.21717171717171718  Loss: 1.1880977581313346e-05\n",
      "\n",
      "Training model  47\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5113813281059265 gradientNorm: 1.0186369411871776\n",
      "epoch: 1000 currentLoss: 0.09726615995168686 gradientNorm: 0.3869178608512814\n",
      "epoch: 2000 currentLoss: 0.0005442826077342033 gradientNorm: 0.00798143750529248\n",
      "epoch: 3000 currentLoss: 3.3244061341974884e-05 gradientNorm: 0.0011074380806850013\n",
      "epoch: 3044 Minimal Ratio: 0.2041847041847042  Loss: 3.0539664294337854e-05\n",
      "\n",
      "Training model  48\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.43115681409835815 gradientNorm: 0.6739199407804025\n",
      "epoch: 1000 currentLoss: 0.05836863070726395 gradientNorm: 0.27993143627628014\n",
      "epoch: 2000 currentLoss: 0.0003201905346941203 gradientNorm: 0.01263479774663886\n",
      "epoch: 2846 Minimal Ratio: 0.2222222222222222  Loss: 1.451485604775371e-05\n",
      "\n",
      "Training model  49\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.36378124356269836 gradientNorm: 0.3873970105400022\n",
      "epoch: 1000 currentLoss: 0.07742924988269806 gradientNorm: 0.31584742574891195\n",
      "epoch: 2000 currentLoss: 0.04453679919242859 gradientNorm: 0.20672937256342\n",
      "epoch: 3000 currentLoss: 0.029108839109539986 gradientNorm: 0.15584330307804142\n",
      "epoch: 4000 currentLoss: 0.0186823308467865 gradientNorm: 0.1160125382795383\n",
      "epoch: 5000 currentLoss: 0.011560345068573952 gradientNorm: 0.08424513514020261\n",
      "epoch: 6000 currentLoss: 0.006813310086727142 gradientNorm: 0.05911058930905731\n",
      "epoch: 7000 currentLoss: 0.003773922100663185 gradientNorm: 0.039877531361321626\n",
      "epoch: 8000 currentLoss: 0.0019398824078962207 gradientNorm: 0.02563172337391635\n",
      "epoch: 9000 currentLoss: 0.0009139111498370767 gradientNorm: 0.015490600913348085\n",
      "epoch: 10000 currentLoss: 0.0003904901968780905 gradientNorm: 0.009411806259361145\n",
      "epoch: 11000 currentLoss: 0.00014957846724428236 gradientNorm: 0.004746846705437962\n",
      "epoch: 12000 currentLoss: 5.1353054004721344e-05 gradientNorm: 0.002699193514637719\n",
      "epoch: 13000 currentLoss: 1.586561484145932e-05 gradientNorm: 0.001102629096783814\n",
      "epoch: 13036 Minimal Ratio: 0.22294372294372294  Loss: 1.5186974451353308e-05\n",
      "\n",
      "Training model  50\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4328010678291321 gradientNorm: 0.8474713727793869\n",
      "epoch: 1000 currentLoss: 0.012514235451817513 gradientNorm: 0.14155800547447772\n",
      "epoch: 2000 currentLoss: 5.056854206486605e-05 gradientNorm: 0.002190973036119635\n",
      "epoch: 2420 Minimal Ratio: 0.27344877344877344  Loss: 1.9816452550003305e-05\n",
      "\n",
      "Training model  51\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.378639280796051 gradientNorm: 0.4954023504598286\n",
      "epoch: 1000 currentLoss: 0.04894201457500458 gradientNorm: 0.2442857687043473\n",
      "epoch: 2000 currentLoss: 0.00019181007519364357 gradientNorm: 0.005144230356503515\n",
      "epoch: 2770 Minimal Ratio: 0.23376623376623376  Loss: 4.422606434673071e-05\n",
      "\n",
      "Training model  52\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.33622387051582336 gradientNorm: 0.05556620826355873\n",
      "epoch: 1000 currentLoss: 0.025172965601086617 gradientNorm: 0.24323905341047813\n",
      "epoch: 2000 currentLoss: 0.00013778702123090625 gradientNorm: 0.0020849480650319235\n",
      "epoch: 3000 currentLoss: 5.507142486749217e-05 gradientNorm: 0.0012449943710749498\n",
      "epoch: 3191 Minimal Ratio: 0.2554112554112554  Loss: 4.399894896778278e-05\n",
      "\n",
      "Training model  53\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3686588406562805 gradientNorm: 0.4404177657783091\n",
      "epoch: 1000 currentLoss: 0.04381202161312103 gradientNorm: 0.20672226820628933\n",
      "epoch: 2000 currentLoss: 0.0037093600258231163 gradientNorm: 0.06438122133310875\n",
      "epoch: 3000 currentLoss: 0.00011570227798074484 gradientNorm: 0.002695522339527297\n",
      "epoch: 3663 Minimal Ratio: 0.22727272727272727  Loss: 7.978272333275527e-05\n",
      "\n",
      "Training model  54\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.45622488856315613 gradientNorm: 0.7025870088995363\n",
      "epoch: 1000 currentLoss: 0.3966160714626312 gradientNorm: 0.4941784086596079\n",
      "epoch: 2000 currentLoss: 0.36341366171836853 gradientNorm: 0.33377035991488374\n",
      "epoch: 3000 currentLoss: 0.34553101658821106 gradientNorm: 0.19968020531787847\n",
      "epoch: 4000 currentLoss: 0.3379001319408417 gradientNorm: 0.0966878030241451\n",
      "epoch: 5000 currentLoss: 0.335817813873291 gradientNorm: 0.031925771860629816\n",
      "epoch: 6000 currentLoss: 0.33556997776031494 gradientNorm: 0.005285746931863734\n",
      "epoch: 6619 Minimal Ratio: 0.0007215007215007215  Loss: 0.3355632424354553\n",
      "\n",
      "Training model  55\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4247876703739166 gradientNorm: 0.6163861477272073\n",
      "epoch: 1000 currentLoss: 0.2647872865200043 gradientNorm: 0.6791460580234154\n",
      "epoch: 2000 currentLoss: 0.20116272568702698 gradientNorm: 0.565921557531298\n",
      "epoch: 3000 currentLoss: 0.15431489050388336 gradientNorm: 0.47433567281297\n",
      "epoch: 4000 currentLoss: 0.1172560527920723 gradientNorm: 0.39502299577381006\n",
      "epoch: 5000 currentLoss: 0.08761763572692871 gradientNorm: 0.32528395098919705\n",
      "epoch: 6000 currentLoss: 0.06405723839998245 gradientNorm: 0.26400124995095015\n",
      "epoch: 7000 currentLoss: 0.04560749605298042 gradientNorm: 0.21050505556628513\n",
      "epoch: 8000 currentLoss: 0.03146574646234512 gradientNorm: 0.16423442338478902\n",
      "epoch: 9000 currentLoss: 0.020917881280183792 gradientNorm: 0.12502152835329533\n",
      "epoch: 10000 currentLoss: 0.01330981682986021 gradientNorm: 0.09225114283174478\n",
      "epoch: 11000 currentLoss: 0.00804144423455 gradientNorm: 0.06576994262261057\n",
      "epoch: 12000 currentLoss: 0.004569754004478455 gradientNorm: 0.04529464049009553\n",
      "epoch: 13000 currentLoss: 0.0024164856877177954 gradientNorm: 0.0290265767119868\n",
      "epoch: 14000 currentLoss: 0.0011744428193196654 gradientNorm: 0.018126240235740904\n",
      "epoch: 15000 currentLoss: 0.0005186421913094819 gradientNorm: 0.010657514755401925\n",
      "epoch: 16000 currentLoss: 0.0002060648548649624 gradientNorm: 0.005634869979782869\n",
      "epoch: 17000 currentLoss: 7.34028362785466e-05 gradientNorm: 0.00328674089715977\n",
      "epoch: 18000 currentLoss: 2.3547769160359167e-05 gradientNorm: 0.0023374229823824575\n",
      "epoch: 18344 Minimal Ratio: 0.20057720057720058  Loss: 1.5541620086878538e-05\n",
      "\n",
      "Training model  56\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.38975343108177185 gradientNorm: 0.5267617511530371\n",
      "epoch: 1000 currentLoss: 0.09244532883167267 gradientNorm: 0.3724043792835073\n",
      "epoch: 2000 currentLoss: 0.0005681683542206883 gradientNorm: 0.007365579235710543\n",
      "epoch: 3000 currentLoss: 7.286995969479904e-05 gradientNorm: 0.0014356038375549812\n",
      "epoch: 3247 Minimal Ratio: 0.25036075036075034  Loss: 4.69108963443432e-05\n",
      "\n",
      "Training model  57\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.467124879360199 gradientNorm: 0.8209283367421424\n",
      "epoch: 1000 currentLoss: 0.04458005353808403 gradientNorm: 0.2315525912963773\n",
      "epoch: 2000 currentLoss: 0.02625277265906334 gradientNorm: 0.14530345550084262\n",
      "epoch: 3000 currentLoss: 0.01776491478085518 gradientNorm: 0.11213415378239407\n",
      "epoch: 4000 currentLoss: 0.011273670010268688 gradientNorm: 0.08279097637789115\n",
      "epoch: 5000 currentLoss: 0.006721011828631163 gradientNorm: 0.05864901218340661\n",
      "epoch: 6000 currentLoss: 0.0037444077897816896 gradientNorm: 0.03971165866300586\n",
      "epoch: 7000 currentLoss: 0.0019315355457365513 gradientNorm: 0.025547817376625034\n",
      "epoch: 8000 currentLoss: 0.0009113712003454566 gradientNorm: 0.015584068656482266\n",
      "epoch: 9000 currentLoss: 0.0003886485646944493 gradientNorm: 0.008790448845638862\n",
      "epoch: 10000 currentLoss: 0.00014886508870404214 gradientNorm: 0.004814437295533456\n",
      "epoch: 11000 currentLoss: 5.1728311518672854e-05 gradientNorm: 0.0046342398518046165\n",
      "epoch: 12000 currentLoss: 1.6079839042504318e-05 gradientNorm: 0.0011885541796338246\n",
      "epoch: 12030 Minimal Ratio: 0.24891774891774893  Loss: 1.5496694686589763e-05\n",
      "\n",
      "Training model  58\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4415162205696106 gradientNorm: 0.751355828688142\n",
      "epoch: 1000 currentLoss: 0.09804189950227737 gradientNorm: 0.34260486096327464\n",
      "epoch: 2000 currentLoss: 0.00667002284899354 gradientNorm: 0.08597344163301114\n",
      "epoch: 3000 currentLoss: 0.00023123745631892234 gradientNorm: 0.008241823385662692\n",
      "epoch: 4000 currentLoss: 2.2032689230400138e-05 gradientNorm: 0.0012399215912838606\n",
      "epoch: 4085 Minimal Ratio: 0.22582972582972582  Loss: 1.900991628644988e-05\n",
      "\n",
      "Training model  59\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.38980796933174133 gradientNorm: 0.5153535134549786\n",
      "epoch: 1000 currentLoss: 0.03210391849279404 gradientNorm: 0.22549665817496017\n",
      "epoch: 2000 currentLoss: 0.0005972192739136517 gradientNorm: 0.008634904571496004\n",
      "epoch: 3000 currentLoss: 0.00013431911065708846 gradientNorm: 0.0030273831505004743\n",
      "epoch: 4000 currentLoss: 3.0818213417660445e-05 gradientNorm: 0.0013579919142159098\n",
      "epoch: 4039 Minimal Ratio: 0.25685425685425683  Loss: 2.927259265561588e-05\n",
      "\n",
      "Training model  60\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.38998275995254517 gradientNorm: 0.5104647307168351\n",
      "epoch: 1000 currentLoss: 0.1949111372232437 gradientNorm: 0.5519816060595708\n",
      "epoch: 2000 currentLoss: 0.1397554725408554 gradientNorm: 0.44383545049177964\n",
      "epoch: 3000 currentLoss: 0.10323304682970047 gradientNorm: 0.36274428260894703\n",
      "epoch: 4000 currentLoss: 0.07561486214399338 gradientNorm: 0.29477378378270436\n",
      "epoch: 5000 currentLoss: 0.05431312695145607 gradientNorm: 0.23641546729206553\n",
      "epoch: 6000 currentLoss: 0.037984155118465424 gradientNorm: 0.18628146715518082\n",
      "epoch: 7000 currentLoss: 0.025698235258460045 gradientNorm: 0.1435072404343156\n",
      "epoch: 8000 currentLoss: 0.01670791022479534 gradientNorm: 0.10757476807340877\n",
      "epoch: 9000 currentLoss: 0.010360103100538254 gradientNorm: 0.07826556903970035\n",
      "epoch: 10000 currentLoss: 0.006072498857975006 gradientNorm: 0.05471335435493673\n",
      "epoch: 11000 currentLoss: 0.003329798812046647 gradientNorm: 0.03642554985684267\n",
      "epoch: 12000 currentLoss: 0.0016886459197849035 gradientNorm: 0.02346292465880218\n",
      "epoch: 13000 currentLoss: 0.0007823696359992027 gradientNorm: 0.01392114638842553\n",
      "epoch: 14000 currentLoss: 0.00032759434543550014 gradientNorm: 0.007691317856216459\n",
      "epoch: 15000 currentLoss: 0.00012326192518230528 gradientNorm: 0.004370711304837851\n",
      "epoch: 16000 currentLoss: 4.1556682845111936e-05 gradientNorm: 0.0022282652560223245\n",
      "epoch: 16842 Minimal Ratio: 0.24891774891774893  Loss: 1.5382494893856347e-05\n",
      "\n",
      "Training model  61\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.40375107526779175 gradientNorm: 0.542353878023719\n",
      "epoch: 1000 currentLoss: 0.042440060526132584 gradientNorm: 0.38817165905784157\n",
      "epoch: 2000 currentLoss: 1.6314981621690094e-05 gradientNorm: 0.0010337405081624212\n",
      "epoch: 2010 Minimal Ratio: 0.24386724386724387  Loss: 1.5892903320491314e-05\n",
      "\n",
      "Training model  62\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5964578986167908 gradientNorm: 1.2570506844200195\n",
      "epoch: 1000 currentLoss: 0.006392918061465025 gradientNorm: 0.12328288360135356\n",
      "epoch: 2000 currentLoss: 8.737903408473358e-05 gradientNorm: 0.002048296779953685\n",
      "epoch: 2710 Minimal Ratio: 0.23088023088023088  Loss: 3.0170846002874896e-05\n",
      "\n",
      "Training model  63\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.40398284792900085 gradientNorm: 0.5337410405291614\n",
      "epoch: 1000 currentLoss: 0.07563149183988571 gradientNorm: 0.28520241836059473\n",
      "epoch: 2000 currentLoss: 0.05138706788420677 gradientNorm: 0.227198189868526\n",
      "epoch: 3000 currentLoss: 0.034880660474300385 gradientNorm: 0.1757973994002281\n",
      "epoch: 4000 currentLoss: 0.02305917628109455 gradientNorm: 0.13344596256674837\n",
      "epoch: 5000 currentLoss: 0.01468781940639019 gradientNorm: 0.0988321540348431\n",
      "epoch: 6000 currentLoss: 0.008924592286348343 gradientNorm: 0.07091984485578752\n",
      "epoch: 7000 currentLoss: 0.005119030829519033 gradientNorm: 0.048958723777443605\n",
      "epoch: 8000 currentLoss: 0.002740278607234359 gradientNorm: 0.03230264334986274\n",
      "epoch: 9000 currentLoss: 0.0013523149536922574 gradientNorm: 0.02001834367268687\n",
      "epoch: 10000 currentLoss: 0.0006076979334466159 gradientNorm: 0.012121403546808936\n",
      "epoch: 11000 currentLoss: 0.00024610370746813715 gradientNorm: 0.006526498282960367\n",
      "epoch: 12000 currentLoss: 8.927821181714535e-05 gradientNorm: 0.003337979822922217\n",
      "epoch: 13000 currentLoss: 2.905199653469026e-05 gradientNorm: 0.0015937936491033236\n",
      "epoch: 13603 Minimal Ratio: 0.2554112554112554  Loss: 1.4057927728572395e-05\n",
      "\n",
      "Training model  64\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4760682284832001 gradientNorm: 0.8118575156159356\n",
      "epoch: 1000 currentLoss: 0.012588786892592907 gradientNorm: 0.12367175834190272\n",
      "epoch: 2000 currentLoss: 8.789190178504214e-05 gradientNorm: 0.0041235782300070275\n",
      "epoch: 2758 Minimal Ratio: 0.2943722943722944  Loss: 1.6425870853709057e-05\n",
      "\n",
      "Training model  65\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3884592652320862 gradientNorm: 0.5736706161194289\n",
      "epoch: 1000 currentLoss: 0.00877336785197258 gradientNorm: 0.09655557463302476\n",
      "epoch: 2000 currentLoss: 0.00011088136670878157 gradientNorm: 0.004140780883691543\n",
      "epoch: 2798 Minimal Ratio: 0.22438672438672438  Loss: 1.132531906478107e-05\n",
      "\n",
      "Training model  66\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3729887902736664 gradientNorm: 0.39533080029662243\n",
      "epoch: 1000 currentLoss: 0.22218948602676392 gradientNorm: 0.546930657934137\n",
      "epoch: 2000 currentLoss: 0.14868168532848358 gradientNorm: 0.46216545053745955\n",
      "epoch: 3000 currentLoss: 0.10969829559326172 gradientNorm: 0.3777210339192896\n",
      "epoch: 4000 currentLoss: 0.08058564364910126 gradientNorm: 0.3075867393254798\n",
      "epoch: 5000 currentLoss: 0.05815700441598892 gradientNorm: 0.24748629372390013\n",
      "epoch: 6000 currentLoss: 0.040917232632637024 gradientNorm: 0.19560733112527828\n",
      "epoch: 7000 currentLoss: 0.027885185554623604 gradientNorm: 0.15174100928175832\n",
      "epoch: 8000 currentLoss: 0.018287209793925285 gradientNorm: 0.11457086837568158\n",
      "epoch: 9000 currentLoss: 0.011455683968961239 gradientNorm: 0.08375709539325188\n",
      "epoch: 10000 currentLoss: 0.006796190515160561 gradientNorm: 0.05924245660156895\n",
      "epoch: 11000 currentLoss: 0.0037803184241056442 gradientNorm: 0.03983638748990439\n",
      "epoch: 12000 currentLoss: 0.0019494271837174892 gradientNorm: 0.025441149663692788\n",
      "epoch: 13000 currentLoss: 0.0009208287810906768 gradientNorm: 0.015612782944176947\n",
      "epoch: 14000 currentLoss: 0.0003937612345907837 gradientNorm: 0.008783948397983342\n",
      "epoch: 15000 currentLoss: 0.0001511739392299205 gradientNorm: 0.00472288092289594\n",
      "epoch: 16000 currentLoss: 5.196420170250349e-05 gradientNorm: 0.002274353489421745\n",
      "epoch: 17000 currentLoss: 1.609484024811536e-05 gradientNorm: 0.0011587441295238955\n",
      "epoch: 17052 Minimal Ratio: 0.2741702741702742  Loss: 1.5103790246939752e-05\n",
      "\n",
      "Training model  67\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3633858561515808 gradientNorm: 0.41625295820472974\n",
      "epoch: 1000 currentLoss: 0.15474192798137665 gradientNorm: 0.4062846877009485\n",
      "epoch: 2000 currentLoss: 0.0972164198756218 gradientNorm: 0.34689057158977105\n",
      "epoch: 3000 currentLoss: 0.06856140494346619 gradientNorm: 0.27577536504081934\n",
      "epoch: 4000 currentLoss: 0.04810016602277756 gradientNorm: 0.21791059677566318\n",
      "epoch: 5000 currentLoss: 0.0329991951584816 gradientNorm: 0.1695644575941408\n",
      "epoch: 6000 currentLoss: 0.021917512640357018 gradientNorm: 0.12917297022004276\n",
      "epoch: 7000 currentLoss: 0.013972275890409946 gradientNorm: 0.09553739257649825\n",
      "epoch: 8000 currentLoss: 0.008473961614072323 gradientNorm: 0.06850002864207252\n",
      "epoch: 9000 currentLoss: 0.0048415204510092735 gradientNorm: 0.0468003622933504\n",
      "epoch: 10000 currentLoss: 0.0025770878419280052 gradientNorm: 0.03095301437080697\n",
      "epoch: 11000 currentLoss: 0.0012627309188246727 gradientNorm: 0.019190321383678297\n",
      "epoch: 12000 currentLoss: 0.0005627207574434578 gradientNorm: 0.011547404530572661\n",
      "epoch: 13000 currentLoss: 0.00022575266484636813 gradientNorm: 0.006285930380353495\n",
      "epoch: 14000 currentLoss: 8.109015470836312e-05 gradientNorm: 0.003153210728104547\n",
      "epoch: 15000 currentLoss: 2.615424273244571e-05 gradientNorm: 0.0014753762856812036\n",
      "epoch: 15465 Minimal Ratio: 0.21356421356421357  Loss: 1.4930007637303788e-05\n",
      "\n",
      "Training model  68\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.46427688002586365 gradientNorm: 0.7501618984834149\n",
      "epoch: 1000 currentLoss: 0.3863399624824524 gradientNorm: 0.5096375860818015\n",
      "epoch: 2000 currentLoss: 0.0020439510699361563 gradientNorm: 0.017138234594667227\n",
      "epoch: 3000 currentLoss: 0.0001521296362625435 gradientNorm: 0.003969889623699382\n",
      "epoch: 4000 currentLoss: 2.656892320374027e-05 gradientNorm: 0.0010233980847774195\n",
      "epoch: 4008 Minimal Ratio: 0.23088023088023088  Loss: 2.6262312530889176e-05\n",
      "\n",
      "Training model  69\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4085109829902649 gradientNorm: 0.540177698258524\n",
      "epoch: 1000 currentLoss: 0.36749163269996643 gradientNorm: 0.3573718009850917\n",
      "epoch: 2000 currentLoss: 0.34651535749435425 gradientNorm: 0.20930704715069723\n",
      "epoch: 3000 currentLoss: 0.3379954397678375 gradientNorm: 0.09863948799431525\n",
      "epoch: 4000 currentLoss: 0.33580565452575684 gradientNorm: 0.031155807502855588\n",
      "epoch: 5000 currentLoss: 0.3355686366558075 gradientNorm: 0.004757192477361077\n",
      "epoch: 5562 Minimal Ratio: 0.0007215007215007215  Loss: 0.33556321263313293\n",
      "\n",
      "Training model  70\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4632972776889801 gradientNorm: 0.777363177836158\n",
      "epoch: 1000 currentLoss: 0.10659890621900558 gradientNorm: 0.37561044733283677\n",
      "epoch: 2000 currentLoss: 0.0011182662565261126 gradientNorm: 0.01428629810851355\n",
      "epoch: 3000 currentLoss: 7.318386633414775e-05 gradientNorm: 0.0017763333185444046\n",
      "epoch: 3377 Minimal Ratio: 0.20346320346320346  Loss: 3.843640661216341e-05\n",
      "\n",
      "Training model  71\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.43599745631217957 gradientNorm: 0.7767089758003206\n",
      "epoch: 1000 currentLoss: 0.04042545706033707 gradientNorm: 0.20820784471361983\n",
      "epoch: 2000 currentLoss: 0.001157047925516963 gradientNorm: 0.011680633074776412\n",
      "epoch: 3000 currentLoss: 0.00018050882499665022 gradientNorm: 0.002537916343704476\n",
      "epoch: 3944 Minimal Ratio: 0.24242424242424243  Loss: 7.743466994725168e-05\n",
      "\n",
      "Training model  72\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.33547383546829224 gradientNorm: 0.017444642263493113\n",
      "epoch: 1000 currentLoss: 0.02455013245344162 gradientNorm: 0.16646000368628552\n",
      "epoch: 2000 currentLoss: 9.419108209840488e-06 gradientNorm: 0.0011009925887928685\n",
      "epoch: 2034 Minimal Ratio: 0.25901875901875904  Loss: 8.1929138104897e-06\n",
      "\n",
      "Training model  73\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.33559104800224304 gradientNorm: 0.012711433774735471\n",
      "epoch: 1000 currentLoss: 0.0020976343657821417 gradientNorm: 0.04263737702780134\n",
      "epoch: 2000 currentLoss: 0.00011914917558897287 gradientNorm: 0.002492291473953964\n",
      "epoch: 2898 Minimal Ratio: 0.3189033189033189  Loss: 2.2109275960247032e-05\n",
      "\n",
      "Training model  74\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.38123002648353577 gradientNorm: 0.5765053349110215\n",
      "epoch: 1000 currentLoss: 0.0033473276998847723 gradientNorm: 0.06355911129078877\n",
      "epoch: 2000 currentLoss: 8.832450112095103e-05 gradientNorm: 0.002873409521666001\n",
      "epoch: 2966 Minimal Ratio: 0.23953823953823955  Loss: 1.4107656170381233e-05\n",
      "\n",
      "Training model  75\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.39914920926094055 gradientNorm: 0.611881751817544\n",
      "epoch: 1000 currentLoss: 0.06755366176366806 gradientNorm: 0.45199993896611246\n",
      "epoch: 2000 currentLoss: 0.0005428539589047432 gradientNorm: 0.004452405606623275\n",
      "epoch: 3000 currentLoss: 9.765835420694202e-05 gradientNorm: 0.0018056219424783032\n",
      "epoch: 3442 Minimal Ratio: 0.19985569985569984  Loss: 3.4400356526020914e-05\n",
      "\n",
      "Training model  76\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.39746415615081787 gradientNorm: 0.5356347331661855\n",
      "epoch: 1000 currentLoss: 0.08536320924758911 gradientNorm: 0.36897528223932347\n",
      "epoch: 2000 currentLoss: 0.04685043543577194 gradientNorm: 0.21410868885435572\n",
      "epoch: 3000 currentLoss: 0.031121816486120224 gradientNorm: 0.16303708602433675\n",
      "epoch: 4000 currentLoss: 0.020210830494761467 gradientNorm: 0.12231037619945907\n",
      "epoch: 5000 currentLoss: 0.01264821644872427 gradientNorm: 0.08949314113188936\n",
      "epoch: 6000 currentLoss: 0.007539045065641403 gradientNorm: 0.06340165866616643\n",
      "epoch: 7000 currentLoss: 0.0042305816896259785 gradientNorm: 0.04311882008379604\n",
      "epoch: 8000 currentLoss: 0.002208069199696183 gradientNorm: 0.02803720727601427\n",
      "epoch: 9000 currentLoss: 0.0010584734845906496 gradientNorm: 0.016995206963952573\n",
      "epoch: 10000 currentLoss: 0.00046039989683777094 gradientNorm: 0.009824496982307149\n",
      "epoch: 11000 currentLoss: 0.00017999984265770763 gradientNorm: 0.005178399261039416\n",
      "epoch: 12000 currentLoss: 6.321367982309312e-05 gradientNorm: 0.006110937651983999\n",
      "epoch: 13000 currentLoss: 1.98348152480321e-05 gradientNorm: 0.0012048768658876091\n",
      "epoch: 13215 Minimal Ratio: 0.22943722943722944  Loss: 1.5263665773090906e-05\n",
      "\n",
      "Training model  77\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.343538761138916 gradientNorm: 0.20511360834442702\n",
      "epoch: 1000 currentLoss: 0.05370762571692467 gradientNorm: 0.26599168572139265\n",
      "epoch: 2000 currentLoss: 0.00012291167513467371 gradientNorm: 0.004056965394411497\n",
      "epoch: 2449 Minimal Ratio: 0.2647907647907648  Loss: 3.1976949685486034e-05\n",
      "\n",
      "Training model  78\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3387477993965149 gradientNorm: 0.12720756704171815\n",
      "epoch: 1000 currentLoss: 0.11661650240421295 gradientNorm: 0.3908275585622406\n",
      "epoch: 2000 currentLoss: 0.07466452568769455 gradientNorm: 0.29168068475827325\n",
      "epoch: 3000 currentLoss: 0.05135093629360199 gradientNorm: 0.22751356054173397\n",
      "epoch: 4000 currentLoss: 0.03500903397798538 gradientNorm: 0.1763383219213191\n",
      "epoch: 5000 currentLoss: 0.02324218489229679 gradientNorm: 0.13423234841914827\n",
      "epoch: 6000 currentLoss: 0.014859253540635109 gradientNorm: 0.0996182964596944\n",
      "epoch: 7000 currentLoss: 0.009058174677193165 gradientNorm: 0.07161784639923223\n",
      "epoch: 8000 currentLoss: 0.0052118063904345036 gradientNorm: 0.04950333661512671\n",
      "epoch: 9000 currentLoss: 0.002798737259581685 gradientNorm: 0.03295965776998314\n",
      "epoch: 10000 currentLoss: 0.0013858169550076127 gradientNorm: 0.020498553019626484\n",
      "epoch: 11000 currentLoss: 0.0006250630831345916 gradientNorm: 0.011951244586055544\n",
      "epoch: 12000 currentLoss: 0.00025409128284081817 gradientNorm: 0.0066370659897978775\n",
      "epoch: 13000 currentLoss: 9.251196024706587e-05 gradientNorm: 0.0033233150638666497\n",
      "epoch: 14000 currentLoss: 3.0299124773591757e-05 gradientNorm: 0.003523001201943121\n",
      "epoch: 14581 Minimal Ratio: 0.1847041847041847  Loss: 1.5028732377686538e-05\n",
      "\n",
      "Training model  79\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.33717218041419983 gradientNorm: 0.10283238389759582\n",
      "epoch: 1000 currentLoss: 0.08452323824167252 gradientNorm: 0.28439812548604976\n",
      "epoch: 2000 currentLoss: 0.006523176562041044 gradientNorm: 0.07343397758897678\n",
      "epoch: 3000 currentLoss: 0.00013884603686165065 gradientNorm: 0.00641918632406055\n",
      "epoch: 4000 currentLoss: 2.5247980374842882e-05 gradientNorm: 0.0011828768293440895\n",
      "epoch: 4167 Minimal Ratio: 0.15873015873015872  Loss: 1.972767677216325e-05\n",
      "\n",
      "Training model  80\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3366008996963501 gradientNorm: 0.0868766897581052\n",
      "epoch: 1000 currentLoss: 0.004389157053083181 gradientNorm: 0.04090027829487035\n",
      "epoch: 2000 currentLoss: 0.00033579402952454984 gradientNorm: 0.005220620420016604\n",
      "epoch: 3000 currentLoss: 8.675256685819477e-05 gradientNorm: 0.0023659983124399214\n",
      "epoch: 3542 Minimal Ratio: 0.2611832611832612  Loss: 4.3615666072582826e-05\n",
      "\n",
      "Training model  81\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4689118564128876 gradientNorm: 0.7900930817616966\n",
      "epoch: 1000 currentLoss: 0.001882988028228283 gradientNorm: 0.024121873122152304\n",
      "epoch: 2000 currentLoss: 0.00019902082567568868 gradientNorm: 0.0031959767604253795\n",
      "epoch: 3000 currentLoss: 7.468976400559768e-05 gradientNorm: 0.0016047021264226572\n",
      "epoch: 3315 Minimal Ratio: 0.2344877344877345  Loss: 5.0062597438227385e-05\n",
      "\n",
      "Training model  82\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3449662923812866 gradientNorm: 0.19394111782428167\n",
      "epoch: 1000 currentLoss: 0.33612120151519775 gradientNorm: 0.04725468859766358\n",
      "epoch: 2000 currentLoss: 0.3355657160282135 gradientNorm: 0.0033090091145024283\n",
      "epoch: 2307 Minimal Ratio: 0.0007215007215007215  Loss: 0.3355632424354553\n",
      "\n",
      "Training model  83\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5735655426979065 gradientNorm: 1.1123796861819872\n",
      "epoch: 1000 currentLoss: 0.39002934098243713 gradientNorm: 0.8367899003759629\n",
      "epoch: 2000 currentLoss: 0.3037808835506439 gradientNorm: 0.7447283044561681\n",
      "epoch: 3000 currentLoss: 0.2407943159341812 gradientNorm: 0.637972130894011\n",
      "epoch: 4000 currentLoss: 0.189372256398201 gradientNorm: 0.5436774759443422\n",
      "epoch: 5000 currentLoss: 0.1469661444425583 gradientNorm: 0.4591190466238683\n",
      "epoch: 6000 currentLoss: 0.11213015019893646 gradientNorm: 0.38337159744703636\n",
      "epoch: 7000 currentLoss: 0.08382344245910645 gradientNorm: 0.3157892625192563\n",
      "epoch: 8000 currentLoss: 0.061180394142866135 gradientNorm: 0.2559099100923031\n",
      "epoch: 9000 currentLoss: 0.043424710631370544 gradientNorm: 0.2038019966877004\n",
      "epoch: 10000 currentLoss: 0.029833590611815453 gradientNorm: 0.15864547314219485\n",
      "epoch: 11000 currentLoss: 0.01972738839685917 gradientNorm: 0.12042916127688949\n",
      "epoch: 12000 currentLoss: 0.012472067959606647 gradientNorm: 0.08830643424012034\n",
      "epoch: 13000 currentLoss: 0.007477501407265663 gradientNorm: 0.06279389394110571\n",
      "epoch: 14000 currentLoss: 0.0042106993496418 gradientNorm: 0.04281767563737523\n",
      "epoch: 15000 currentLoss: 0.002202561590820551 gradientNorm: 0.02838733986051792\n",
      "epoch: 16000 currentLoss: 0.0010572766186669469 gradientNorm: 0.0172279056297986\n",
      "epoch: 17000 currentLoss: 0.0004603606357704848 gradientNorm: 0.00960018744716768\n",
      "epoch: 18000 currentLoss: 0.00018016390094999224 gradientNorm: 0.0052096405686172485\n",
      "epoch: 19000 currentLoss: 6.311539618764073e-05 gradientNorm: 0.00365586672122093\n",
      "epoch: 20000 Minimal Ratio: 0.22077922077922077  Loss: 1.9878712919307873e-05\n",
      "\n",
      "Training model  84\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.39994752407073975 gradientNorm: 0.6614226137759223\n",
      "epoch: 1000 currentLoss: 0.04109436646103859 gradientNorm: 0.2651077487285088\n",
      "epoch: 2000 currentLoss: 8.26923715067096e-05 gradientNorm: 0.00304135535919428\n",
      "epoch: 2535 Minimal Ratio: 0.2481962481962482  Loss: 2.0208770365570672e-05\n",
      "\n",
      "Training model  85\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.36521416902542114 gradientNorm: 0.4151854707411209\n",
      "epoch: 1000 currentLoss: 0.06884724646806717 gradientNorm: 0.2712401727702261\n",
      "epoch: 2000 currentLoss: 0.04386114329099655 gradientNorm: 0.20359650482019553\n",
      "epoch: 3000 currentLoss: 0.02878401428461075 gradientNorm: 0.15283531924255073\n",
      "epoch: 4000 currentLoss: 0.018507737666368484 gradientNorm: 0.11397558291773346\n",
      "epoch: 5000 currentLoss: 0.011452884413301945 gradientNorm: 0.08288668963958219\n",
      "epoch: 6000 currentLoss: 0.0067360177636146545 gradientNorm: 0.05824034770078282\n",
      "epoch: 7000 currentLoss: 0.003719709813594818 gradientNorm: 0.039312619794098204\n",
      "epoch: 8000 currentLoss: 0.001905166544020176 gradientNorm: 0.025135627934641278\n",
      "epoch: 9000 currentLoss: 0.0008937414386309683 gradientNorm: 0.015258710735357416\n",
      "epoch: 10000 currentLoss: 0.0003796685195993632 gradientNorm: 0.008712046835251562\n",
      "epoch: 11000 currentLoss: 0.0001448629191145301 gradientNorm: 0.004644135830377265\n",
      "epoch: 12000 currentLoss: 4.954768519382924e-05 gradientNorm: 0.0024434561622112385\n",
      "epoch: 12999 Minimal Ratio: 0.2012987012987013  Loss: 1.530211193312425e-05\n",
      "\n",
      "Training model  86\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.4392758309841156 gradientNorm: 0.7139002642199128\n",
      "epoch: 1000 currentLoss: 0.0668649896979332 gradientNorm: 0.2957326634249478\n",
      "epoch: 2000 currentLoss: 0.0010289703495800495 gradientNorm: 0.020041395347533016\n",
      "epoch: 3000 currentLoss: 2.2355834516929463e-05 gradientNorm: 0.001207227312117424\n",
      "epoch: 3094 Minimal Ratio: 0.27994227994227994  Loss: 1.848533247539308e-05\n",
      "\n",
      "Training model  87\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.48468494415283203 gradientNorm: 0.7945204590954014\n",
      "epoch: 1000 currentLoss: 0.19234232604503632 gradientNorm: 0.7605801666201952\n",
      "epoch: 2000 currentLoss: 0.04406485706567764 gradientNorm: 0.20463312194498345\n",
      "epoch: 3000 currentLoss: 0.031774524599313736 gradientNorm: 0.16518388312655813\n",
      "epoch: 4000 currentLoss: 0.0216895192861557 gradientNorm: 0.12814320203720514\n",
      "epoch: 5000 currentLoss: 0.014030721969902515 gradientNorm: 0.0959321906622392\n",
      "epoch: 6000 currentLoss: 0.00858281645923853 gradientNorm: 0.06919518178452036\n",
      "epoch: 7000 currentLoss: 0.004932388197630644 gradientNorm: 0.047552065379979\n",
      "epoch: 8000 currentLoss: 0.0026373635046184063 gradientNorm: 0.03144538505199769\n",
      "epoch: 9000 currentLoss: 0.0012976422440260649 gradientNorm: 0.019602518454578326\n",
      "epoch: 10000 currentLoss: 0.0005807728739455342 gradientNorm: 0.01170400346901044\n",
      "epoch: 11000 currentLoss: 0.0002341194194741547 gradientNorm: 0.0062560301954769516\n",
      "epoch: 12000 currentLoss: 8.453587361145765e-05 gradientNorm: 0.003199220363463735\n",
      "epoch: 13000 currentLoss: 2.7421581762610003e-05 gradientNorm: 0.0015177018582343163\n",
      "epoch: 13541 Minimal Ratio: 0.29653679653679654  Loss: 1.4308661775430664e-05\n",
      "\n",
      "Training model  88\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3397091031074524 gradientNorm: 0.16859446300833003\n",
      "epoch: 1000 currentLoss: 0.012236958369612694 gradientNorm: 0.09583308239083654\n",
      "epoch: 2000 currentLoss: 0.00011996948160231113 gradientNorm: 0.006217519095080889\n",
      "epoch: 2884 Minimal Ratio: 0.2518037518037518  Loss: 1.0200446013186593e-05\n",
      "\n",
      "Training model  89\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3364802300930023 gradientNorm: 0.06738238487379505\n",
      "epoch: 1000 currentLoss: 0.04067818820476532 gradientNorm: 0.20088294539605545\n",
      "epoch: 2000 currentLoss: 0.0007402956252917647 gradientNorm: 0.014461395252335451\n",
      "epoch: 3000 currentLoss: 0.0001182028790935874 gradientNorm: 0.004561829709312793\n",
      "epoch: 4000 currentLoss: 1.6192936527659185e-05 gradientNorm: 0.001145642034234558\n",
      "epoch: 4086 Minimal Ratio: 0.2712842712842713  Loss: 1.4069159078644589e-05\n",
      "\n",
      "Training model  90\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3668440580368042 gradientNorm: 0.3801607388980587\n",
      "epoch: 1000 currentLoss: 0.2622802257537842 gradientNorm: 0.3750696469003853\n",
      "epoch: 2000 currentLoss: 0.13060447573661804 gradientNorm: 0.4168028423447718\n",
      "epoch: 3000 currentLoss: 0.09387434273958206 gradientNorm: 0.34028250431805246\n",
      "epoch: 4000 currentLoss: 0.06767049431800842 gradientNorm: 0.27359463391423167\n",
      "epoch: 5000 currentLoss: 0.047951824963092804 gradientNorm: 0.21752507597074738\n",
      "epoch: 6000 currentLoss: 0.033076927065849304 gradientNorm: 0.1699520039839014\n",
      "epoch: 7000 currentLoss: 0.0220436230301857 gradientNorm: 0.12948221250895\n",
      "epoch: 8000 currentLoss: 0.014086026698350906 gradientNorm: 0.09599454494963638\n",
      "epoch: 9000 currentLoss: 0.008559579961001873 gradientNorm: 0.0694920870260821\n",
      "epoch: 10000 currentLoss: 0.00489930622279644 gradientNorm: 0.04723986191666628\n",
      "epoch: 11000 currentLoss: 0.002612748648971319 gradientNorm: 0.031216479438593195\n",
      "epoch: 12000 currentLoss: 0.0012828897451981902 gradientNorm: 0.020098544183753633\n",
      "epoch: 13000 currentLoss: 0.0005729202530346811 gradientNorm: 0.011430135682237628\n",
      "epoch: 14000 currentLoss: 0.00023073072952684015 gradientNorm: 0.010015587648791225\n",
      "epoch: 15000 currentLoss: 8.294310100609437e-05 gradientNorm: 0.003139247145528201\n",
      "epoch: 16000 currentLoss: 2.6818650439963676e-05 gradientNorm: 0.002086393212655847\n",
      "epoch: 16476 Minimal Ratio: 0.20274170274170275  Loss: 1.5098295989446342e-05\n",
      "\n",
      "Training model  91\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5081285834312439 gradientNorm: 1.0612100260130524\n",
      "epoch: 1000 currentLoss: 0.4114176630973816 gradientNorm: 0.5508345449648798\n",
      "epoch: 2000 currentLoss: 0.3730410039424896 gradientNorm: 0.38718474854360674\n",
      "epoch: 3000 currentLoss: 0.35074925422668457 gradientNorm: 0.2464653418880767\n",
      "epoch: 4000 currentLoss: 0.33995985984802246 gradientNorm: 0.13261770942147072\n",
      "epoch: 5000 currentLoss: 0.3362632393836975 gradientNorm: 0.05292442522600083\n",
      "epoch: 6000 currentLoss: 0.33560073375701904 gradientNorm: 0.012289639739466693\n",
      "epoch: 7000 currentLoss: 0.3355633020401001 gradientNorm: 0.00109970755047112\n",
      "epoch: 7032 Minimal Ratio: 0.0007215007215007215  Loss: 0.33556321263313293\n",
      "\n",
      "Training model  92\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.443502813577652 gradientNorm: 0.7276075615228214\n",
      "epoch: 1000 currentLoss: 0.10683450102806091 gradientNorm: 0.38183701971911704\n",
      "epoch: 2000 currentLoss: 0.011982209049165249 gradientNorm: 0.10667856245071693\n",
      "epoch: 3000 currentLoss: 3.750189353013411e-05 gradientNorm: 0.0013424846448531437\n",
      "epoch: 3130 Minimal Ratio: 0.25901875901875904  Loss: 2.5779510906431824e-05\n",
      "\n",
      "Training model  93\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5627790093421936 gradientNorm: 1.0255821503581273\n",
      "epoch: 1000 currentLoss: 0.04520581290125847 gradientNorm: 0.2251267173757985\n",
      "epoch: 2000 currentLoss: 0.014368594624102116 gradientNorm: 0.11496141761517376\n",
      "epoch: 3000 currentLoss: 2.166701597161591e-05 gradientNorm: 0.0020355015425849573\n",
      "epoch: 3238 Minimal Ratio: 0.25757575757575757  Loss: 8.982378858490847e-06\n",
      "\n",
      "Training model  94\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3895004093647003 gradientNorm: 0.5562936620379851\n",
      "epoch: 1000 currentLoss: 0.06128782778978348 gradientNorm: 0.2816791486682518\n",
      "epoch: 2000 currentLoss: 0.0037940701004117727 gradientNorm: 0.037133938684466655\n",
      "epoch: 3000 currentLoss: 0.0008109790505841374 gradientNorm: 0.01989364555589532\n",
      "epoch: 4000 currentLoss: 0.00016124776448123157 gradientNorm: 0.004860198092311554\n",
      "epoch: 5000 currentLoss: 3.07855116261635e-05 gradientNorm: 0.0015993205321794732\n",
      "epoch: 5353 Minimal Ratio: 0.20057720057720058  Loss: 1.7077705706469715e-05\n",
      "\n",
      "Training model  95\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.5236003398895264 gradientNorm: 0.884884547045946\n",
      "epoch: 1000 currentLoss: 0.4382716417312622 gradientNorm: 0.6688989082190109\n",
      "epoch: 2000 currentLoss: 0.0036927806213498116 gradientNorm: 0.029823788692456495\n",
      "epoch: 3000 currentLoss: 0.00039172518881969154 gradientNorm: 0.004541969900606661\n",
      "epoch: 4000 currentLoss: 6.798128742957488e-05 gradientNorm: 0.001715487244464107\n",
      "epoch: 4471 Minimal Ratio: 0.2907647907647908  Loss: 3.222215673304163e-05\n",
      "\n",
      "Training model  96\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3355470299720764 gradientNorm: 0.049458162141986936\n",
      "epoch: 1000 currentLoss: 0.01151104923337698 gradientNorm: 0.1109475414370976\n",
      "epoch: 2000 currentLoss: 5.3614730859408155e-05 gradientNorm: 0.0021561245334315326\n",
      "epoch: 2495 Minimal Ratio: 0.2453102453102453  Loss: 2.082903665723279e-05\n",
      "\n",
      "Training model  97\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3360684812068939 gradientNorm: 0.06463260252989166\n",
      "epoch: 1000 currentLoss: 0.025147562846541405 gradientNorm: 0.15371768118143025\n",
      "epoch: 2000 currentLoss: 0.0019482759525999427 gradientNorm: 0.013710526822859831\n",
      "epoch: 3000 currentLoss: 0.00020844576647505164 gradientNorm: 0.006786644786047677\n",
      "epoch: 4000 currentLoss: 1.8588030798127875e-05 gradientNorm: 0.0013340349420343917\n",
      "epoch: 4142 Minimal Ratio: 0.2287157287157287  Loss: 1.38799741762341e-05\n",
      "\n",
      "Training model  98\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.44838374853134155 gradientNorm: 0.8840086007745661\n",
      "epoch: 1000 currentLoss: 0.03368502855300903 gradientNorm: 0.22892547590107026\n",
      "epoch: 2000 currentLoss: 0.003457065438851714 gradientNorm: 0.05343939507921226\n",
      "epoch: 3000 currentLoss: 3.485820707282983e-05 gradientNorm: 0.0024926631315447113\n",
      "epoch: 3330 Minimal Ratio: 0.22943722943722944  Loss: 1.107445677916985e-05\n",
      "\n",
      "Training model  99\n",
      "Training for 20000 epochs until Loss goes below 0.001\n",
      "epoch: 0 currentLoss: 0.3495377004146576 gradientNorm: 0.3029112298017899\n",
      "epoch: 1000 currentLoss: 0.0594470389187336 gradientNorm: 0.29081763098877955\n",
      "epoch: 2000 currentLoss: 0.00014490445028059185 gradientNorm: 0.006007311481019475\n",
      "epoch: 2364 Minimal Ratio: 0.25901875901875904  Loss: 1.2841629541071597e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "min_ratio_arr = []\n",
    "for index in range(100):\n",
    "    print(\"Training model \", index)\n",
    "    model = Model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "\n",
    "    min_ratio, loss_value = train(model, func1_x, func1_x)\n",
    "    loss_arr.append(loss_value)\n",
    "    min_ratio_arr.append(min_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47f0a561",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "47f0a561",
    "outputId": "1c736dbc-66e2-4fdc-b995-310c4757a7aa",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAG0CAYAAADacZikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzjklEQVR4nO3df3hU1YHH/88kIQlQZooBEighRrSGAAIJQn5sxO6WANUuqN8vqaxRqxayaktMbUuKugG2jdiyAkoQWiGl+yXECq48u7EQulWCibjGhKctqVILJqUTQ1AyoJJAcr9/REaHSUISJpkM5/16nvvAnDn33HNPTpgPZ+7csVmWZQkAAMAgQf7uAAAAQH8jAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4/g9ABUUFCg2Nlbh4eFKTExUWVlZp3UPHDig1NRURUREaPDgwYqLi9PTTz/tUaewsFA2m81rO3v2bF+fCgAACBAh/jx4cXGxsrOzVVBQoNTUVG3atEnz5s3T4cOHNW7cOK/6Q4cO1cMPP6wbbrhBQ4cO1YEDB7RkyRINHTpUixcvdtez2+165513PPYNDw/vdr/a2tr097//XcOGDZPNZuv9CQIAgH5jWZZOnz6tMWPGKCjoEms8lh/NmDHDysrK8iiLi4uzli1b1u02brvtNuuuu+5yP966davlcDguq191dXWWJDY2NjY2NrYA3Orq6i75Wu+3FaCWlhZVVlZq2bJlHuXp6ekqLy/vVhtVVVUqLy/Xv//7v3uUnzlzRjExMWptbdXUqVO1atUqTZs2rdN2mpub1dzc7H5sWZYkqa6uTna7vbunBAAA/Mjlcik6OlrDhg27ZF2/BaDGxka1trYqMjLSozwyMlL19fVd7jt27FidOHFC58+fV15enh544AH3c3FxcSosLNTkyZPlcrm0bt06paam6tChQ7ruuus6bC8/P18rVqzwKrfb7QQgAAACTHcuX/HrNUCSdycty7pkx8vKynTmzBm98cYbWrZsma699lrdeeedkqSkpCQlJSW566ampiohIUHPPPOM1q9f32F7ubm5ysnJcT++kCABAMCVyW8BaMSIEQoODvZa7WloaPBaFbpYbGysJGny5Mn64IMPlJeX5w5AFwsKCtKNN96oI0eOdNpeWFiYwsLCengGAAAgUPntY/ChoaFKTExUaWmpR3lpaalSUlK63Y5lWR7X73T0fHV1tUaPHt3rvgIAgCuLX98Cy8nJUWZmpqZPn67k5GRt3rxZtbW1ysrKktT+1tTx48e1bds2SdKGDRs0btw4xcXFSWq/L9DPf/5zffe733W3uWLFCiUlJem6666Ty+XS+vXrVV1drQ0bNvT/CQIAgAHJrwEoIyNDJ0+e1MqVK+V0OjVp0iSVlJQoJiZGkuR0OlVbW+uu39bWptzcXB09elQhISEaP368nnzySS1ZssRd59SpU1q8eLHq6+vlcDg0bdo07d+/XzNmzOj38wMAAAOTzbrwmW+4uVwuORwONTU18SkwAAACRE9ev/3+VRgAAAD9jQAEAACMQwACAADG8fuNEI1y5rj0ymTp3Glp0DBp3h+kL33F370CAMA4BKD+siNMamv5/PG5j6TdY6WgUOlbnd/HCAAA+B5vgfWHi8PPF7W1tD8PAAD6DQGor5053nn4uaCtpb0eAADoFwSgvvbKZN/WAwAAl40A1NfOnfZtPQAAcNkIQH1t0DDf1gMAAJeNANTX5v3Bt/UAAMBlIwD1tS99pf2j7l0JCuV+QAAA9CMCUH/4VnPnIYj7AAEA0O+4EWJ/+VYzd4IGAGCAIAD1py99Rfp/P/R3LwAAMB5vgQEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjEIAAAIBxCEAAAMA4BCAAAGAcAhAAADAOAQgAABiHAAQAAIxDAAIAAMbxewAqKChQbGyswsPDlZiYqLKysk7rHjhwQKmpqYqIiNDgwYMVFxenp59+2qvezp07FR8fr7CwMMXHx+ull17qy1MAAAABxq8BqLi4WNnZ2Vq+fLmqqqqUlpamefPmqba2tsP6Q4cO1cMPP6z9+/erpqZGjz32mB577DFt3rzZXaeiokIZGRnKzMzUoUOHlJmZqYULF+rgwYP9dVoAAGCAs1mWZfnr4DNnzlRCQoI2btzoLpswYYIWLFig/Pz8brVx++23a+jQofr1r38tScrIyJDL5dIrr7zirjN37lwNHz5cRUVF3WrT5XLJ4XCoqalJdru9B2cEAAD8pSev335bAWppaVFlZaXS09M9ytPT01VeXt6tNqqqqlReXq5Zs2a5yyoqKrzanDNnTrfbBAAAV74Qfx24sbFRra2tioyM9CiPjIxUfX19l/uOHTtWJ06c0Pnz55WXl6cHHnjA/Vx9fX2P22xublZzc7P7scvl6smpAACAAOP3i6BtNpvHY8uyvMouVlZWprfeekvPPfec1q5d6/XWVk/bzM/Pl8PhcG/R0dE9PAsAABBI/LYCNGLECAUHB3utzDQ0NHit4FwsNjZWkjR58mR98MEHysvL05133ilJioqK6nGbubm5ysnJcT92uVyEIAAArmB+WwEKDQ1VYmKiSktLPcpLS0uVkpLS7XYsy/J4+yo5Odmrzb1793bZZlhYmOx2u8cGAACuXH5bAZKknJwcZWZmavr06UpOTtbmzZtVW1urrKwsSe0rM8ePH9e2bdskSRs2bNC4ceMUFxcnqf2+QD//+c/13e9+193m0qVLddNNN2n16tWaP3++Xn75Ze3bt08HDhzo/xMEAAADkl8DUEZGhk6ePKmVK1fK6XRq0qRJKikpUUxMjCTJ6XR63BOora1Nubm5Onr0qEJCQjR+/Hg9+eSTWrJkibtOSkqKduzYoccee0yPP/64xo8fr+LiYs2cObPfzw8AAAxMfr0P0EDFfYAAAAg8AXEfIAAAAH8hAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjEIAAAIBxCEAAAMA4BCAAAGAcAhAAADAOAQgAABiHAAQAAIxDAAIAAMYhAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYx+8BqKCgQLGxsQoPD1diYqLKyso6rbtr1y7Nnj1bI0eOlN1uV3Jysvbs2eNRp7CwUDabzWs7e/ZsX58KAAAIEH4NQMXFxcrOztby5ctVVVWltLQ0zZs3T7W1tR3W379/v2bPnq2SkhJVVlbqa1/7mr75zW+qqqrKo57dbpfT6fTYwsPD++OUAABAALBZlmX56+AzZ85UQkKCNm7c6C6bMGGCFixYoPz8/G61MXHiRGVkZOiJJ56Q1L4ClJ2drVOnTvW6Xy6XSw6HQ01NTbLb7b1uBwAA9J+evH77bQWopaVFlZWVSk9P9yhPT09XeXl5t9poa2vT6dOnddVVV3mUnzlzRjExMRo7dqxuvfVWrxUiAABgNr8FoMbGRrW2tioyMtKjPDIyUvX19d1qY82aNfr444+1cOFCd1lcXJwKCwu1e/duFRUVKTw8XKmpqTpy5Ein7TQ3N8vlcnlsAADgyhXi7w7YbDaPx5ZleZV1pKioSHl5eXr55Zc1atQod3lSUpKSkpLcj1NTU5WQkKBnnnlG69ev77Ct/Px8rVixopdnAAAAAo3fVoBGjBih4OBgr9WehoYGr1WhixUXF+v+++/XCy+8oK9//etd1g0KCtKNN97Y5QpQbm6umpqa3FtdXV33TwQAAAQcvwWg0NBQJSYmqrS01KO8tLRUKSkpne5XVFSke++9V9u3b9ctt9xyyeNYlqXq6mqNHj260zphYWGy2+0eGwAAuHL59S2wnJwcZWZmavr06UpOTtbmzZtVW1urrKwsSe0rM8ePH9e2bdsktYefu+++W+vWrVNSUpJ79Wjw4MFyOBySpBUrVigpKUnXXXedXC6X1q9fr+rqam3YsME/JwkAAAYcvwagjIwMnTx5UitXrpTT6dSkSZNUUlKimJgYSZLT6fS4J9CmTZt0/vx5PfTQQ3rooYfc5ffcc48KCwslSadOndLixYtVX18vh8OhadOmaf/+/ZoxY0a/nhsAABi4/HofoIGK+wABABB4AuI+QAAAAP5CAAIAAMYhAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjEIAAAIBxCEAAAMA4BCAAAGAcAhAAADAOAQgAABiHAAQAAIxDAAIAAMYhAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwjt8DUEFBgWJjYxUeHq7ExESVlZV1WnfXrl2aPXu2Ro4cKbvdruTkZO3Zs8er3s6dOxUfH6+wsDDFx8frpZde6stTAAAAAcavAai4uFjZ2dlavny5qqqqlJaWpnnz5qm2trbD+vv379fs2bNVUlKiyspKfe1rX9M3v/lNVVVVuetUVFQoIyNDmZmZOnTokDIzM7Vw4UIdPHiwv04LAAAMcDbLsix/HXzmzJlKSEjQxo0b3WUTJkzQggULlJ+f3602Jk6cqIyMDD3xxBOSpIyMDLlcLr3yyivuOnPnztXw4cNVVFTUrTZdLpccDoeamppkt9t7cEYAAMBfevL67bcVoJaWFlVWVio9Pd2jPD09XeXl5d1qo62tTadPn9ZVV13lLquoqPBqc86cOd1uEwAAXPlC/HXgxsZGtba2KjIy0qM8MjJS9fX13WpjzZo1+vjjj7Vw4UJ3WX19fY/bbG5uVnNzs/uxy+Xq1vEBAEBg8vtF0DabzeOxZVleZR0pKipSXl6eiouLNWrUqMtqMz8/Xw6Hw71FR0f34AwAAECg8VsAGjFihIKDg71WZhoaGrxWcC5WXFys+++/Xy+88IK+/vWvezwXFRXV4zZzc3PV1NTk3urq6np4NgAAIJD4LQCFhoYqMTFRpaWlHuWlpaVKSUnpdL+ioiLde++92r59u2655Rav55OTk73a3Lt3b5dthoWFyW63e2wAAODK5bdrgCQpJydHmZmZmj59upKTk7V582bV1tYqKytLUvvKzPHjx7Vt2zZJ7eHn7rvv1rp165SUlORe6Rk8eLAcDockaenSpbrpppu0evVqzZ8/Xy+//LL27dunAwcO+OckAQDAgOPXa4AyMjK0du1arVy5UlOnTtX+/ftVUlKimJgYSZLT6fS4J9CmTZt0/vx5PfTQQxo9erR7W7p0qbtOSkqKduzYoa1bt+qGG25QYWGhiouLNXPmzH4/PwAAMDD59T5AAxX3AQIAIPAExH2AAAAA/IUABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjEIAAAIBxCEAAAMA4vQpAdXV1+tvf/uZ+/Oabbyo7O1ubN2/2WccAAAD6Sq8C0KJFi/T73/9eklRfX6/Zs2frzTff1I9//GOtXLnSpx0EAADwtV4FoD/+8Y+aMWOGJOmFF17QpEmTVF5eru3bt6uwsNCX/QMAAPC5XgWgc+fOKSwsTJK0b98+/fM//7MkKS4uTk6n03e9AwAA6AO9CkATJ07Uc889p7KyMpWWlmru3LmSpL///e+KiIjwaQcBAAB8rVcBaPXq1dq0aZNuvvlm3XnnnZoyZYokaffu3e63xgAAAAYqm2VZVm92bG1tlcvl0vDhw91lx44d05AhQzRq1CifddAfXC6XHA6HmpqaZLfb/d0dAADQDT15/e7VCtCnn36q5uZmd/h5//33tXbtWr3zzjsBH34AAMCVr1cBaP78+dq2bZsk6dSpU5o5c6bWrFmjBQsWaOPGjT7tIAAAgK/1KgC9/fbbSktLkyS9+OKLioyM1Pvvv69t27Zp/fr1Pu0gAACAr/UqAH3yyScaNmyYJGnv3r26/fbbFRQUpKSkJL3//vs+7SAAAICv9SoAXXvttfqv//ov1dXVac+ePUpPT5ckNTQ0cNEwAAAY8HoVgJ544gk9+uijuvrqqzVjxgwlJydLal8NmjZtmk87CAAA4Gu9/hh8fX29nE6npkyZoqCg9hz15ptvym63Ky4uzqed7G98DB4AgMDTk9fvkN4eJCoqSlFRUfrb3/4mm82mr3zlK9wEEQAABIRevQXW1tamlStXyuFwKCYmRuPGjdOXv/xlrVq1Sm1tbb7uIwAAgE/1agVo+fLlev755/Xkk08qNTVVlmXp9ddfV15ens6ePauf/OQnvu4nAACAz/TqGqAxY8boueeec38L/AUvv/yyHnzwQR0/ftxnHfQHrgECACDw9PlXYXz44YcdXugcFxenDz/8sDdNAgAA9JteBaApU6bo2Wef9Sp/9tlndcMNN1x2pwAAAPpSr64Beuqpp3TLLbdo3759Sk5Ols1mU3l5uerq6lRSUuLrPgIAAPhUr1aAZs2apXfffVe33XabTp06pQ8//FC33367/vSnP2nr1q2+7iMAAIBP9fpGiB05dOiQEhIS1Nra6qsm/YKLoAEACDx9fhE0AABAICMAAQAA4/g9ABUUFCg2Nlbh4eFKTExUWVlZp3WdTqcWLVqk66+/XkFBQcrOzvaqU1hYKJvN5rWdPXu2D88CAAAEkh59Cuz222/v8vlTp0716ODFxcXKzs5WQUGBUlNTtWnTJs2bN0+HDx/WuHHjvOo3Nzdr5MiRWr58uZ5++ulO27Xb7XrnnXc8ysLDw3vUNwAAcOXqUQByOByXfP7uu+/udnv/8R//ofvvv18PPPCAJGnt2rXas2ePNm7cqPz8fK/6V199tdatWydJ2rJlS6ft2mw2RUVFdbsfAADALD0KQL78iHtLS4sqKyu1bNkyj/L09HSVl5dfVttnzpxRTEyMWltbNXXqVK1atUrTpk27rDYBAMCVw2/XADU2Nqq1tVWRkZEe5ZGRkaqvr+91u3FxcSosLNTu3btVVFSk8PBwpaam6siRI53u09zcLJfL5bEBAIArl98vgrbZbB6PLcvyKuuJpKQk3XXXXZoyZYrS0tL0wgsv6Ktf/aqeeeaZTvfJz8+Xw+Fwb9HR0b0+PgAAGPj8FoBGjBih4OBgr9WehoYGr1WhyxEUFKQbb7yxyxWg3NxcNTU1ube6ujqfHR8AAAw8fgtAoaGhSkxMVGlpqUd5aWmpUlJSfHYcy7JUXV2t0aNHd1onLCxMdrvdYwMAAFeuXn0Zqq/k5OQoMzNT06dPV3JysjZv3qza2lplZWVJal+ZOX78uLZt2+bep7q6WlL7hc4nTpxQdXW1QkNDFR8fL0lasWKFkpKSdN1118nlcmn9+vWqrq7Whg0b+v38AADAwOTXAJSRkaGTJ09q5cqVcjqdmjRpkkpKShQTEyOp/caHtbW1Hvt88dNclZWV2r59u2JiYnTs2DFJ7fciWrx4serr6+VwODRt2jTt379fM2bM6LfzAgAAA5tPvwz1SsGXoQIAEHj4MlQAAIAuEIAAAIBxCEAAAMA4BCAAAGAcAhAAADAOAQgAABiHAAQAAIxDAAIAAMYhAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjEIAAAIBxCEAAAMA4BCAAAGAcAhAAADAOAQgAABiHAAQAAIxDAAIAAMYhAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjOP3AFRQUKDY2FiFh4crMTFRZWVlndZ1Op1atGiRrr/+egUFBSk7O7vDejt37lR8fLzCwsIUHx+vl156qY96DwAAApFfA1BxcbGys7O1fPlyVVVVKS0tTfPmzVNtbW2H9ZubmzVy5EgtX75cU6ZM6bBORUWFMjIylJmZqUOHDikzM1MLFy7UwYMH+/JUAABAALFZlmX56+AzZ85UQkKCNm7c6C6bMGGCFixYoPz8/C73vfnmmzV16lStXbvWozwjI0Mul0uvvPKKu2zu3LkaPny4ioqKutUvl8slh8OhpqYm2e327p8QAADwm568fvttBailpUWVlZVKT0/3KE9PT1d5eXmv262oqPBqc86cOZfVJgAAuLKE+OvAjY2Nam1tVWRkpEd5ZGSk6uvre91ufX19j9tsbm5Wc3Oz+7HL5er18QEAwMDn94ugbTabx2PLsrzK+rrN/Px8ORwO9xYdHX1ZxwcAAAOb3wLQiBEjFBwc7LUy09DQ4LWC0xNRUVE9bjM3N1dNTU3ura6urtfHBwAAA5/fAlBoaKgSExNVWlrqUV5aWqqUlJRet5ucnOzV5t69e7tsMywsTHa73WMDAABXLr9dAyRJOTk5yszM1PTp05WcnKzNmzertrZWWVlZktpXZo4fP65t27a596murpYknTlzRidOnFB1dbVCQ0MVHx8vSVq6dKluuukmrV69WvPnz9fLL7+sffv26cCBA/1+fgAAYGDyawDKyMjQyZMntXLlSjmdTk2aNEklJSWKiYmR1H7jw4vvCTRt2jT33ysrK7V9+3bFxMTo2LFjkqSUlBTt2LFDjz32mB5//HGNHz9excXFmjlzZr+dFwAAGNj8eh+ggYr7AAEAEHgC4j5AAAAA/kIAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjEIAAAIBxCEAAAMA4BCAAAGAcAhAAADAOAQgAABiHAAQAAIxDAAIAAMYhAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjEIAAAIBxCEAAAMA4BCAAAGAcAhAAADCO3wNQQUGBYmNjFR4ersTERJWVlXVZ/7XXXlNiYqLCw8N1zTXX6LnnnvN4vrCwUDabzWs7e/ZsX54GAAAIIH4NQMXFxcrOztby5ctVVVWltLQ0zZs3T7W1tR3WP3r0qL7xjW8oLS1NVVVV+vGPf6zvfe972rlzp0c9u90up9PpsYWHh/fHKQEAgABgsyzL8tfBZ86cqYSEBG3cuNFdNmHCBC1YsED5+fle9X/0ox9p9+7dqqmpcZdlZWXp0KFDqqiokNS+ApSdna1Tp071ul8ul0sOh0NNTU2y2+29bgcAAPSfnrx++20FqKWlRZWVlUpPT/coT09PV3l5eYf7VFRUeNWfM2eO3nrrLZ07d85ddubMGcXExGjs2LG69dZbVVVV5fsTAAAAActvAaixsVGtra2KjIz0KI+MjFR9fX2H+9TX13dY//z582psbJQkxcXFqbCwULt371ZRUZHCw8OVmpqqI0eOdNqX5uZmuVwujw0AAFy5/H4RtM1m83hsWZZX2aXqf7E8KSlJd911l6ZMmaK0tDS98MIL+upXv6pnnnmm0zbz8/PlcDjcW3R0dG9PBwAABAC/BaARI0YoODjYa7WnoaHBa5XngqioqA7rh4SEKCIiosN9goKCdOONN3a5ApSbm6umpib3VldX18OzAQAAgcRvASg0NFSJiYkqLS31KC8tLVVKSkqH+yQnJ3vV37t3r6ZPn65BgwZ1uI9lWaqurtbo0aM77UtYWJjsdrvHBgAArlx+fQssJydHv/zlL7VlyxbV1NTokUceUW1trbKysiS1r8zcfffd7vpZWVl6//33lZOTo5qaGm3ZskXPP/+8Hn30UXedFStWaM+ePfrrX/+q6upq3X///aqurna3CQAAEOLPg2dkZOjkyZNauXKlnE6nJk2apJKSEsXExEiSnE6nxz2BYmNjVVJSokceeUQbNmzQmDFjtH79et1xxx3uOqdOndLixYtVX18vh8OhadOmaf/+/ZoxY0a/nx8AABiY/HofoIGK+wABABB4AuI+QAAAAP5CAAIAAMYhAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjEIAAAIBxCEAAAMA4BCAAAGAcAhAAADAOAQgAABiHAAQAAIxDAAIAAMYhAAEAAOMQgAAAgHEIQAAAwDgEIAAAYBwCEAAAMA4BCAAAGIcABAAAjEMAAgAAxiEAAQAA4xCAAACAcQhAAADAOAQgAABgHAIQAAAwToi/O1BQUKCf/exncjqdmjhxotauXau0tLRO67/22mvKycnRn/70J40ZM0Y//OEPlZWV5VFn586devzxx/Xee+9p/Pjx+slPfqLbbrutr08FQHe1tUonyqRPndLg0dLINCkoOPCO4Y/+XGjnk+PS2Q+kTxukjyql1k+lIeOkL0+Vzn0offi2FDxEirxJuu5hKSS06z6Fj5IsSc0Nn/dPkj54VWp4tf3vkTdLI9Kkk+WfHb9eOtsoffy+1HxCCg6XFNS+v80mhdilT+o+e26wJJs0eIzkuF6KfUD662apYb/U+ok0PEEKjZDOnZJsQdKoNEnBn/cnIuXz4zafkAYNl06+0d73QcOkmEWSFSTV/n/SOVf7cSJmSuc+ksJGSkO+0n5O51ukQz+QXO9KQWFSyBDp41qpubG9z4OjpS/FSK4/S+dOS1+eLMUukprekT4+Jg0bL12zRPro4Oc/y4gUqbFMqv/f9vMdPFYKu0pq+eizc7m5/dhf7H9ohNRy0rNvF8+HruZMR89Jnj/Ptrb2n90nte19Ch8hhY3q3nEv/rmPutmz/Y5+JhfavFDe2VwfIL+bNsuyrH4/6meKi4uVmZmpgoICpaamatOmTfrlL3+pw4cPa9y4cV71jx49qkmTJuk73/mOlixZotdff10PPvigioqKdMcdd0iSKioqlJaWplWrVum2227TSy+9pCeeeEIHDhzQzJkzu9Uvl8slh8OhpqYm2e12n54zYLy6XVLlUumTv31eNmSslLhOir49cI7hj/501E63BEkTvi9Ne6r7bYVGSG3N0vkz3m2prYfH9wFbsGS1Xl4bQeFS21nf9MezYV16TC5R5+L50NWckbyfC41o/7PlZM+63tFxDy72bifkS+2B8Yvlnf1MLi7/4jH6+HezJ6/ffg1AM2fOVEJCgjZu3OgumzBhghYsWKD8/Hyv+j/60Y+0e/du1dTUuMuysrJ06NAhVVRUSJIyMjLkcrn0yiuvuOvMnTtXw4cPV1FRUbf6RQAC+kjdLqns/1H7UsMX2dr/SHvx8v8R7I9j+KM/nbbTAxN+0B6CfNEW+oCtfT5IXcyZvviZffG4d/RN+5I04VGp5ufqy9/Nnrx+++0aoJaWFlVWVio9Pd2jPD09XeXl5R3uU1FR4VV/zpw5euutt3Tu3Lku63TWJoB+0tba/j+/Dv8B/6ysMru93kA+hj/602U7PVCzRmr51DdtoW9ULpXe+p66nDM+Z7Uf9/++13ftS9Kf/0MD5ndTfgxAjY2Nam1tVWRkpEd5ZGSk6uvrO9ynvr6+w/rnz59XY2Njl3U6a1OSmpub5XK5PDYAPnai7BJv3Vjt10+cKBvYx/BHfy7ZTne1tV//4pO24HtW+8/m0+P9f+hP/iad7cvjWpd4C7Offzc1AD4FZrPZPB5bluVVdqn6F5f3tM38/Hw5HA73Fh0d3e3+A+imT52+reevY/SEr/rjy/6ePuK7tgBf66/fTfkxAI0YMULBwcFeKzMNDQ1eKzgXREVFdVg/JCREERERXdbprE1Jys3NVVNTk3urq6vrzSkB6Mrg0b6t569j9ISv+uPL/g67zndtAb7WX7+b8mMACg0NVWJiokpLSz3KS0tLlZKS0uE+ycnJXvX37t2r6dOna9CgQV3W6axNSQoLC5PdbvfYAPjYyLT2T3uos9VYmzQk+vOP8w7UY/ijP+52LleQNOVnl+gT/MfW/rMZ/BX1+89nyFgp/Ct9eABb+6fDunq+P3835ee3wHJycvTLX/5SW7ZsUU1NjR555BHV1ta67+uTm5uru+++210/KytL77//vnJyclRTU6MtW7bo+eef16OPPuqus3TpUu3du1erV6/Wn//8Z61evVr79u1TdnZ2f58egC8KCv78I7xe/7h/9jhx7eXdD6Q/juGP/rjbucwXxQnfl0IHd9En+F3iOmn6+s8edDJnOnzuctjaj3vj+ktX7W37khSX89nfB8DvpvwcgDIyMrR27VqtXLlSU6dO1f79+1VSUqKYmBhJktPpVG1trbt+bGysSkpK9Oqrr2rq1KlatWqV1q9f774HkCSlpKRox44d2rp1q2644QYVFhaquLi42/cAAtCHom9v/6jrkIv+pzlkrO8+nt4fx/BHf9zt9GYlKOjzj8B31acvCotov/dLR235Q5erB90UPPjy2+hQd8bkEnWGRH8+H7qcMzvbt4ufC4v4/F5APeF13J0dtxPyJe/yzn4mF5dfmOvTnhpQv5t+vQ/QQMV9gIA+xp2guRM0d4LmTtB98LsZMDdCHKgIQAAABJ6AuBEiAACAvxCAAACAcQhAAADAOAQgAABgHAIQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjhPi7AwPRhZtju1wuP/cEAAB014XX7e58yQUBqAOnT5+WJEVHR/u5JwAAoKdOnz4th8PRZR2+C6wDbW1t+vvf/65hw4bJZrP5rF2Xy6Xo6GjV1dXxHWOfYUw6xrh0jHHxxph0jHHxZsKYWJal06dPa8yYMQoK6voqH1aAOhAUFKSxY8f2Wft2u/2KnXy9xZh0jHHpGOPijTHpGOPi7Uofk0ut/FzARdAAAMA4BCAAAGAcAlA/CgsL07/9278pLCzM310ZMBiTjjEuHWNcvDEmHWNcvDEmnrgIGgAAGIcVIAAAYBwCEAAAMA4BCAAAGIcAdBkKCgoUGxur8PBwJSYmqqysrMv6r732mhITExUeHq5rrrlGzz33nFednTt3Kj4+XmFhYYqPj9dLL73UV93vM74el8LCQtlsNq/t7NmzfXkaPtWTMXE6nVq0aJGuv/56BQUFKTs7u8N6ps2V7ozLlTBXpJ6Ny65duzR79myNHDlSdrtdycnJ2rNnj1e9QJ8vvh4TE+fKgQMHlJqaqoiICA0ePFhxcXF6+umnveoF+lzpNgu9smPHDmvQoEHWL37xC+vw4cPW0qVLraFDh1rvv/9+h/X/+te/WkOGDLGWLl1qHT582PrFL35hDRo0yHrxxRfddcrLy63g4GDrpz/9qVVTU2P99Kc/tUJCQqw33nijv07rsvXFuGzdutWy2+2W0+n02AJFT8fk6NGj1ve+9z3rV7/6lTV16lRr6dKlXnVMnCvdGZdAnyuW1fNxWbp0qbV69WrrzTfftN59910rNzfXGjRokPX222+76wT6fOmLMTFxrrz99tvW9u3brT/+8Y/W0aNHrV//+tfWkCFDrE2bNrnrBPpc6QkCUC/NmDHDysrK8iiLi4uzli1b1mH9H/7wh1ZcXJxH2ZIlS6ykpCT344ULF1pz5871qDNnzhzrW9/6lo963ff6Yly2bt1qORwOn/e1v/R0TL5o1qxZHb7QmzhXvqizcQn0uWJZlzcuF8THx1srVqxwPw70+dIXY8JcaXfbbbdZd911l/txoM+VnuAtsF5oaWlRZWWl0tPTPcrT09NVXl7e4T4VFRVe9efMmaO33npL586d67JOZ20ONH01LpJ05swZxcTEaOzYsbr11ltVVVXl+xPoA70Zk+4wca50V6DOFck349LW1qbTp0/rqquucpcF8nzpqzGRmCtVVVUqLy/XrFmz3GWBPFd6igDUC42NjWptbVVkZKRHeWRkpOrr6zvcp76+vsP658+fV2NjY5d1OmtzoOmrcYmLi1NhYaF2796toqIihYeHKzU1VUeOHOmbE/Gh3oxJd5g4V7ojkOeK5JtxWbNmjT7++GMtXLjQXRbI86WvxsTkuTJ27FiFhYVp+vTpeuihh/TAAw+4nwvkudJTfBnqZbj4m+Ity+ry2+M7qn9xeU/bHIh8PS5JSUlKSkpyP5+amqqEhAQ988wzWr9+va+63af64udq4ly5lCthrki9H5eioiLl5eXp5Zdf1qhRo3zS5kDh6zExea6UlZXpzJkzeuONN7Rs2TJde+21uvPOOy+rzUBEAOqFESNGKDg42CsRNzQ0eCXnC6KiojqsHxISooiIiC7rdNbmQNNX43KxoKAg3XjjjQHxP7XejEl3mDhXeiOQ5op0eeNSXFys+++/X7/5zW/09a9/3eO5QJ4vfTUmFzNprsTGxkqSJk+erA8++EB5eXnuABTIc6WneAusF0JDQ5WYmKjS0lKP8tLSUqWkpHS4T3Jyslf9vXv3avr06Ro0aFCXdTprc6Dpq3G5mGVZqq6u1ujRo33T8T7UmzHpDhPnSm8E0lyRej8uRUVFuvfee7V9+3bdcsstXs8H8nzpqzG5mClz5WKWZam5udn9OJDnSo/1+2XXV4gLHz98/vnnrcOHD1vZ2dnW0KFDrWPHjlmWZVnLli2zMjMz3fUvfNz7kUcesQ4fPmw9//zzXh/3fv31163g4GDrySeftGpqaqwnn3wy4D5+2BfjkpeXZ/32t7+13nvvPauqqsr69re/bYWEhFgHDx7s9/PrjZ6OiWVZVlVVlVVVVWUlJiZaixYtsqqqqqw//elP7udNnCuWdelxCfS5Ylk9H5ft27dbISEh1oYNGzw+zn3q1Cl3nUCfL30xJibOlWeffdbavXu39e6771rvvvuutWXLFstut1vLly931wn0udITBKDLsGHDBismJsYKDQ21EhISrNdee8393D333GPNmjXLo/6rr75qTZs2zQoNDbWuvvpqa+PGjV5t/uY3v7Guv/56a9CgQVZcXJy1c+fOvj4Nn/P1uGRnZ1vjxo2zQkNDrZEjR1rp6elWeXl5f5yKz/R0TCR5bTExMR51TJwrlxqXK2GuWFbPxmXWrFkdjss999zj0Wagzxdfj4mJc2X9+vXWxIkTrSFDhlh2u92aNm2aVVBQYLW2tnq0Gehzpbv4NngAAGAcrgECAADGIQABAADjEIAAAIBxCEAAAMA4BCAAAGAcAhAAADAOAQgAABiHAAQAAIxDAALQb26++WZlZ2d3u/6xY8dks9lUXV3dZ33qz+N05tVXX5XNZtOpU6f8cnzARAQgAL127733ymazKSsry+u5Bx98UDabTffee6+7bNeuXVq1alW324+OjpbT6dSkSZN80d3LcvPNN8tms8lmsyk0NFTjx49Xbm6uxxdJdredi0NgSkqKnE6nHA6HD3sMoCsEIACXJTo6Wjt27NCnn37qLjt79qyKioo0btw4j7pXXXWVhg0b1u22g4ODFRUVpZCQEJ/193J85zvfkdPp1F/+8hc99dRT2rBhg/Ly8i673dDQUEVFRclms11+JwF0CwEIwGVJSEjQuHHjtGvXLnfZrl27FB0drWnTpnnUvXj14+qrr9ZPf/pT3XfffRo2bJjGjRunzZs3u5+/+K2pC28V7dmzR9OmTdPgwYP1j//4j2poaNArr7yiCRMmyG63684779Qnn3zibue3v/2t/uEf/kFf/vKXFRERoVtvvVXvvfdej891yJAhioqK0rhx43THHXdo9uzZ2rt3r/v5kydP6s4779TYsWM1ZMgQTZ48WUVFRe7n7733Xr322mtat26dezXp2LFjHb4FtnPnTk2cOFFhYWG6+uqrtWbNmh73F0DnCEAALtu3v/1tbd261f14y5Ytuu+++7q175o1azR9+nRVVVXpwQcf1L/+67/qz3/+c5f75OXl6dlnn1V5ebnq6uq0cOFCrV27Vtu3b9f//M//qLS0VM8884y7/scff6ycnBz93//9n373u98pKChIt912m9ra2np3wpIOHTqk119/XYMGDXKXnT17VomJifrv//5v/fGPf9TixYuVmZmpgwcPSpLWrVun5ORk90qS0+lUdHS0V9uVlZVauHChvvWtb+kPf/iD8vLy9Pjjj6uwsLDX/QVwEX9/HT2AwHXPPfdY8+fPt06cOGGFhYVZR48etY4dO2aFh4dbJ06csObPn2/dc8897vqzZs2yli5d6n4cExNj3XXXXe7HbW1t1qhRo6yNGzdalmVZR48etSRZVVVVlmVZ1u9//3tLkrVv3z73Pvn5+ZYk67333nOXLVmyxJozZ06n/W5oaLAkWX/4wx86PE5HZs2aZQ0aNMgaOnSoFRoaakmygoKCrBdffLHLMfrGN75hff/73+90DL54Xh999JFlWZa1aNEia/bs2R51fvCDH1jx8fFdHgtA97ECBOCyjRgxQrfccot+9atfaevWrbrllls0YsSIbu17ww03uP9us9kUFRWlhoaGbu8TGRmpIUOG6JprrvEo+2Ib7733nhYtWqRrrrlGdrtdsbGxkqTa2tpu9fGCf/mXf1F1dbUqKiq0cOFC3Xfffbrjjjvcz7e2tuonP/mJbrjhBkVEROhLX/qS9u7d2+Pj1NTUKDU11aMsNTVVR44cUWtra4/aAtCxgXFlIYCAd9999+nhhx+WJG3YsKHb+33xLSSpPQRd6q2pL+5js9ku2cY3v/lNRUdH6xe/+IXGjBmjtrY2TZo0SS0tLd3upyQ5HA5de+21kqT//M//1MSJE/X888/r/vvvl9T+dt7TTz+ttWvXavLkyRo6dKiys7N7fBzLsrwuiLYsq0dtAOgaK0AAfGLu3LlqaWlRS0uL5syZ4+/uuJ08eVI1NTV67LHH9E//9E+aMGGCPvroo8tud9CgQfrxj3+sxx57zH3BdVlZmebPn6+77rpLU6ZM0TXXXKMjR4547BcaGnrJVZz4+HgdOHDAo6y8vFxf/epXFRwcfNl9B0AAAuAjwcHBqqmpUU1NzYB6kR4+fLgiIiK0efNm/eUvf9H//u//KicnxydtL1q0SDabTQUFBZKka6+9VqWlpSovL1dNTY2WLFmi+vp6j32uvvpqHTx4UMeOHVNjY2OHq13f//739bvf/U6rVq3Su+++q1/96ld69tln9eijj/qk3wAIQAB8yG63y263+7sbHoKCgrRjxw5VVlZq0qRJeuSRR/Szn/3MJ22Hhobq4Ycf1lNPPaUzZ87o8ccfV0JCgubMmaObb75ZUVFRWrBggcc+jz76qIKDgxUfH6+RI0d2eH1QQkKCXnjhBe3YsUOTJk3SE088oZUrV3rcVBLA5bFZvLEMAAAMwwoQAAAwDgEIAAAYhwAEAACMQwACAADGIQABAADjEIAAAIBxCEAAAMA4BCAAAGAcAhAAADAOAQgAABiHAAQAAIxDAAIAAMb5/wEOj9wBcGVWIQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(min_ratio_arr,loss_arr, color = \"orange\")\n",
    "plt.xlabel(\"Minimal Ratio\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
